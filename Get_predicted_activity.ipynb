{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-22T02:24:44.710063Z",
     "start_time": "2024-07-22T02:24:43.905107Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# import torchvision.transforms as transforms\n",
    "# import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T02:24:46.747604Z",
     "start_time": "2024-07-22T02:24:46.728486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_device():\n",
    "    \"\"\"\n",
    "    Set the device. CUDA if available, otherwise CPU.\n",
    "    \n",
    "    Args: None\n",
    "    \n",
    "    Returns: Nothing   \n",
    "    \"\"\"\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device != \"cuda\":\n",
    "        print(\"This notebook will be running on CPU!\")\n",
    "    else: \n",
    "        print(\"You have CUDA set up! Woohoo!!!\")\n",
    "        \n",
    "    return device\n",
    "DEVICE = set_device()"
   ],
   "id": "1816971feffb433",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have CUDA set up! Woohoo!!!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T02:25:07.304734Z",
     "start_time": "2024-07-22T02:24:48.968418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, requests\n",
    "\n",
    "fname = []\n",
    "for j in range(3):\n",
    "  fname.append('steinmetz_part%d.npz'%j)\n",
    "url = [\"https://osf.io/agvxh/download\"]\n",
    "url.append(\"https://osf.io/uv3mw/download\")\n",
    "url.append(\"https://osf.io/ehmw2/download\")\n",
    "\n",
    "for j in range(len(url)):\n",
    "  if not os.path.isfile(fname[j]):\n",
    "    try:\n",
    "      r = requests.get(url[j])\n",
    "    except requests.ConnectionError:\n",
    "      print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "      if r.status_code != requests.codes.ok:\n",
    "        print(\"!!! Failed to download data !!!\")\n",
    "      else:\n",
    "        with open(fname[j], \"wb\") as fid:\n",
    "          fid.write(r.content)\n",
    "          \n",
    "alldat = np.array([])\n",
    "for j in range(len(fname)):\n",
    "  alldat = np.hstack((alldat,\n",
    "                      np.load('steinmetz_part%d.npz'%j,\n",
    "                              allow_pickle=True)['dat']))"
   ],
   "id": "fa6ff8b8b201cc95",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T02:25:11.276878Z",
     "start_time": "2024-07-22T02:25:11.152170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "basal_ganglia_regions = [\"ACB\", \"CP\", \"GPe\", \"LS\", \"LSc\", \"LSr\", \"MS\", \"OT\", \"SNr\", \"SI\"]\n",
    "visual_cortex_regions = [\"VISa\", \"VISam\", \"VISl\", \"VISp\", \"VISpm\", \"VISrl\"]\n",
    "\n",
    "total_vc_neurons=0\n",
    "total_bg_neurons=0\n",
    "\n",
    "sessions_to_use = []  ##Session indices\n",
    "session_lengths = []  ##The number of trials in each session\n",
    "input_visual_tensor_list = []\n",
    "label_basal_tensor_list = []\n",
    "\n",
    "for session in range(len(alldat)):\n",
    "  num_vis_neurons = 0\n",
    "  num_basal_neurons = 0\n",
    "  vis_neuron_indices = []\n",
    "  basal_neuron_indices = []\n",
    "  for neuron_idx, area in enumerate(alldat[session]['brain_area']):\n",
    "    if area in visual_cortex_regions:  ## Viusal areas\n",
    "      num_vis_neurons += 1\n",
    "      vis_neuron_indices.append(neuron_idx)\n",
    "    if area in basal_ganglia_regions:  ## Basal Ganglia\n",
    "      num_basal_neurons += 1\n",
    "      basal_neuron_indices.append(neuron_idx)\n",
    "          \n",
    "  trial_num = alldat[session]['spks'].shape[1]   \n",
    "  input_visual_recordings = np.empty((num_vis_neurons, trial_num, 250))\n",
    "  label_basal_recordings = np.empty((num_basal_neurons, trial_num, 250))\n",
    "  \n",
    "  \n",
    "  if num_vis_neurons*num_basal_neurons != 0:\n",
    "    sessions_to_use.append(session)\n",
    "    print(\"session \" ,session, \"has both visual and basal!\")\n",
    "    print('The name of the mouse: ' + alldat[session]['mouse_name'])\n",
    "    print('The session time' + alldat[session]['date_exp'])\n",
    "    print(alldat[session]['spks'].shape)\n",
    "\n",
    "    print('The number of neurons being recorded:', len(alldat[session]['spks']))\n",
    "    print('Number of visual neurons: ', num_vis_neurons)\n",
    "    # print('Indices of visual neurons: ', vis_neuron_indices)\n",
    "    \n",
    "    ## To extract the data from alldat[current_session]\n",
    "    for i, v in enumerate(vis_neuron_indices): \n",
    "      input_visual_recordings[i, :, :] = alldat[session]['spks'][v, :, :]\n",
    "    print(\"reorganized visual activities for input: \", input_visual_recordings.shape)\n",
    "    print('Number of basal neurons: ', num_basal_neurons)\n",
    "    # print('Indices of basal neurons: ', basal_neuron_indices)\n",
    "    for i, g in enumerate(basal_neuron_indices):\n",
    "      label_basal_recordings[i, :, :] = alldat[session]['spks'][g, :, :]\n",
    "    print(\"reorganized basal activities for label\", label_basal_recordings.shape)\n",
    "    print('----------------------------------------------------')\n",
    "    \n",
    "    visual_input_tensor = torch.from_numpy(input_visual_recordings).reshape(-1, 250)\n",
    "    input_visual_tensor_list.append(visual_input_tensor)\n",
    "    basal_label_tensor = torch.from_numpy(label_basal_recordings).reshape(-1, 250)\n",
    "    label_basal_tensor_list.append(basal_label_tensor)\n",
    "\n",
    "# input = torch.cat(input_visual_tensor_list, axis = 0)\n",
    "# label = torch.cat(label_basal_tensor_list, axis = 0)\n",
    "# !!!!!! I realized that the firing rate across different sessions have huge variations, so it may not be a good idea to combine them together!!!!!\\\n",
    "\n",
    "print(\"input tensor list: We have\", len(input_visual_tensor_list), \"and their shapes are \", [input_visual_tensor_list[i].shape for i in range(len(input_visual_tensor_list))])\n",
    "print(\"label tensor list: We have\", len(label_basal_tensor_list), \"and their shapes are\", [label_basal_tensor_list[i].shape for i in range(len(label_basal_tensor_list))])\n"
   ],
   "id": "22bf32dfdeb269d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session  0 has both visual and basal!\n",
      "The name of the mouse: Cori\n",
      "The session time2016-12-14\n",
      "(734, 214, 250)\n",
      "The number of neurons being recorded: 734\n",
      "Number of visual neurons:  178\n",
      "reorganized visual activities for input:  (178, 214, 250)\n",
      "Number of basal neurons:  139\n",
      "reorganized basal activities for label (139, 214, 250)\n",
      "----------------------------------------------------\n",
      "session  3 has both visual and basal!\n",
      "The name of the mouse: Forssmann\n",
      "The session time2017-11-01\n",
      "(1769, 249, 250)\n",
      "The number of neurons being recorded: 1769\n",
      "Number of visual neurons:  120\n",
      "reorganized visual activities for input:  (120, 249, 250)\n",
      "Number of basal neurons:  435\n",
      "reorganized basal activities for label (435, 249, 250)\n",
      "----------------------------------------------------\n",
      "session  7 has both visual and basal!\n",
      "The name of the mouse: Hench\n",
      "The session time2017-06-15\n",
      "(1156, 250, 250)\n",
      "The number of neurons being recorded: 1156\n",
      "Number of visual neurons:  111\n",
      "reorganized visual activities for input:  (111, 250, 250)\n",
      "Number of basal neurons:  3\n",
      "reorganized basal activities for label (3, 250, 250)\n",
      "----------------------------------------------------\n",
      "session  8 has both visual and basal!\n",
      "The name of the mouse: Hench\n",
      "The session time2017-06-16\n",
      "(788, 372, 250)\n",
      "The number of neurons being recorded: 788\n",
      "Number of visual neurons:  142\n",
      "reorganized visual activities for input:  (142, 372, 250)\n",
      "Number of basal neurons:  33\n",
      "reorganized basal activities for label (33, 372, 250)\n",
      "----------------------------------------------------\n",
      "session  9 has both visual and basal!\n",
      "The name of the mouse: Hench\n",
      "The session time2017-06-17\n",
      "(1172, 447, 250)\n",
      "The number of neurons being recorded: 1172\n",
      "Number of visual neurons:  340\n",
      "reorganized visual activities for input:  (340, 447, 250)\n",
      "Number of basal neurons:  63\n",
      "reorganized basal activities for label (63, 447, 250)\n",
      "----------------------------------------------------\n",
      "session  12 has both visual and basal!\n",
      "The name of the mouse: Lederberg\n",
      "The session time2017-12-06\n",
      "(983, 300, 250)\n",
      "The number of neurons being recorded: 983\n",
      "Number of visual neurons:  34\n",
      "reorganized visual activities for input:  (34, 300, 250)\n",
      "Number of basal neurons:  23\n",
      "reorganized basal activities for label (23, 300, 250)\n",
      "----------------------------------------------------\n",
      "session  21 has both visual and basal!\n",
      "The name of the mouse: Muller\n",
      "The session time2017-01-07\n",
      "(646, 444, 250)\n",
      "The number of neurons being recorded: 646\n",
      "Number of visual neurons:  133\n",
      "reorganized visual activities for input:  (133, 444, 250)\n",
      "Number of basal neurons:  92\n",
      "reorganized basal activities for label (92, 444, 250)\n",
      "----------------------------------------------------\n",
      "session  26 has both visual and basal!\n",
      "The name of the mouse: Radnitz\n",
      "The session time2017-01-10\n",
      "(563, 253, 250)\n",
      "The number of neurons being recorded: 563\n",
      "Number of visual neurons:  103\n",
      "reorganized visual activities for input:  (103, 253, 250)\n",
      "Number of basal neurons:  90\n",
      "reorganized basal activities for label (90, 253, 250)\n",
      "----------------------------------------------------\n",
      "session  29 has both visual and basal!\n",
      "The name of the mouse: Richards\n",
      "The session time2017-10-29\n",
      "(942, 143, 250)\n",
      "The number of neurons being recorded: 942\n",
      "Number of visual neurons:  19\n",
      "reorganized visual activities for input:  (19, 143, 250)\n",
      "Number of basal neurons:  47\n",
      "reorganized basal activities for label (47, 143, 250)\n",
      "----------------------------------------------------\n",
      "session  34 has both visual and basal!\n",
      "The name of the mouse: Tatum\n",
      "The session time2017-12-06\n",
      "(795, 311, 250)\n",
      "The number of neurons being recorded: 795\n",
      "Number of visual neurons:  75\n",
      "reorganized visual activities for input:  (75, 311, 250)\n",
      "Number of basal neurons:  219\n",
      "reorganized basal activities for label (219, 311, 250)\n",
      "----------------------------------------------------\n",
      "input tensor list: We have 10 and their shapes are  [torch.Size([38092, 250]), torch.Size([29880, 250]), torch.Size([27750, 250]), torch.Size([52824, 250]), torch.Size([151980, 250]), torch.Size([10200, 250]), torch.Size([59052, 250]), torch.Size([26059, 250]), torch.Size([2717, 250]), torch.Size([23325, 250])]\n",
      "label tensor list: We have 10 and their shapes are [torch.Size([29746, 250]), torch.Size([108315, 250]), torch.Size([750, 250]), torch.Size([12276, 250]), torch.Size([28161, 250]), torch.Size([6900, 250]), torch.Size([40848, 250]), torch.Size([22770, 250]), torch.Size([6721, 250]), torch.Size([68109, 250])]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`alldat` contains 39 sessions from 10 mice, data from Steinmetz et al, 2019. Time bins for all measurements are 10ms, starting 500ms before stimulus onset. The mouse had to determine which side has the highest contrast. For each `dat = alldat[k]`, you have the fields below. For extra variables, check out the extra notebook and extra data files (lfp, waveforms and exact spike times, non-binned). \n",
    "\n",
    "* `dat['mouse_name']`: mouse name\n",
    "* `dat['date_exp']`: when a session was performed\n",
    "* `dat['spks']`: neurons by trials by time bins.    \n",
    "* `dat['brain_area']`: brain area for each neuron recorded. \n",
    "* `dat['ccf']`: Allen Institute brain atlas coordinates for each neuron. \n",
    "* `dat['ccf_axes']`: axes names for the Allen CCF. \n",
    "* `dat['contrast_right']`: contrast level for the right stimulus, which is always contralateral to the recorded brain areas.\n",
    "* `dat['contrast_left']`: contrast level for left stimulus. \n",
    "* `dat['gocue']`: when the go cue sound was played. \n",
    "* `dat['response_time']`: when the response was registered, which has to be after the go cue. The mouse can turn the wheel before the go cue (and nearly always does!), but the stimulus on the screen won't move before the go cue.  \n",
    "* `dat['response']`: which side the response was (`-1`, `0`, `1`). When the right-side stimulus had higher contrast, the correct choice was `-1`. `0` is a no go response. \n",
    "* `dat['feedback_time']`: when feedback was provided. \n",
    "* `dat['feedback_type']`: if the feedback was positive (`+1`, reward) or negative (`-1`, white noise burst).  \n",
    "* `dat['wheel']`: turning speed of the wheel that the mice uses to make a response, sampled at `10ms`. \n",
    "* `dat['pupil']`: pupil area  (noisy, because pupil is very small) + pupil horizontal and vertical position.\n",
    "* `dat['face']`: average face motion energy from a video camera. \n",
    "* `dat['licks']`: lick detections, 0 or 1.   \n",
    "* `dat['trough_to_peak']`: measures the width of the action potential waveform for each neuron. Widths `<=10` samples are \"putative fast spiking neurons\". \n",
    "* `dat['%X%_passive']`: same as above for `X` = {`spks`, `pupil`, `wheel`, `contrast_left`, `contrast_right`} but for  passive trials at the end of the recording when the mouse was no longer engaged and stopped making responses. \n",
    "* `dat['prev_reward']`: time of the feedback (reward/white noise) on the previous trial in relation to the current stimulus time. \n",
    "* `dat['reaction_time']`: ntrials by 2. First column: reaction time computed from the wheel movement as the first sample above `5` ticks/10ms bin. Second column: direction of the wheel movement (`0` = no move detected).  \n",
    "\n",
    "\n",
    "The original dataset is here: https://figshare.com/articles/dataset/Dataset_from_Steinmetz_et_al_2019/9598406"
   ],
   "id": "69afd360d00ec9e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T02:25:17.808215Z",
     "start_time": "2024-07-22T02:25:17.720480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 2024/07/15 We set the turncation threshold to be 50 neurons\n",
    "## adding reshape methods -- please go over the code to see if modified/added \"aspects\" are consistent \n",
    "## and do what they are intended to do, the number of neurons went down (perhaps a little bit more than intended)\n",
    "## and the same for the trial number, but now they match in dimensions\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    I tried to make it tailored to our needs.\n",
    "    It only considers sessions that have both visual and basal ganglia recordings.\n",
    "    It only considers go trials.\n",
    "    When indexed, it returns basal ganglia spikes and action).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        visual_cortex_regions: list = [\"VISa\", \"VISam\", \"VISl\", \"VISp\", \"VISpm\", \"VISrl\"], # Put them as default argument so they can be easily changed if we decide to change areas\n",
    "        basal_ganglia_regions: list = [\"ACB\", \"CP\", \"GPe\", \"LS\", \"LSc\", \"LSr\", \"MS\", \"OT\", \"SNr\", \"SI\"],\n",
    "        thresh=50\n",
    "    ) -> None:\n",
    "        \n",
    "        self.visual_cortex_regions = visual_cortex_regions\n",
    "        self.basal_ganglia_regions = basal_ganglia_regions\n",
    "        self.thresh=thresh\n",
    "        self.data = self.visual_and_basal(data)\n",
    "        self.data = self.get_go_trials(self.data)\n",
    "        self.min_trials, self.min_neurons = self.find_min_dimensions(self.data) # new \n",
    "        print(f\"Minimum number of trials: {self.min_trials}\")\n",
    "        print(f\"Minimum number of neurons: {self.thresh}\")\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        session_index: int\n",
    "    ) -> torch.tensor:\n",
    "        \n",
    "        session = self.data[session_index]\n",
    "        \n",
    "        # We need visual spikes now!!!\n",
    "        input_visual_spks = self.get_truncated_spikes(session, self.visual_cortex_regions)\n",
    "        \n",
    "        ## Some number checks by Yangdong 24/7/15\n",
    "        ## print(\"the shape of spikes_visual after truncating:\", spikes_visual.shape)\n",
    "\n",
    "        # input_basal_spks = self.get_truncated_spikes(session, self.basal_ganglia_regions) # This will be the input data (time, trials, eurons_indicies)          \n",
    "        label_action = torch.tensor(session['response'][0:self.min_trials]) #(-1: the mouse is turning the wheel to the left, which indicates that the mouse thinks the right side has higher contrast level. )\n",
    "        \n",
    "        return input_visual_spks, label_action\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.data)\n",
    "\n",
    "    def visual_and_basal(\n",
    "        self,\n",
    "        data: np.array\n",
    "    ) -> list:\n",
    "        \"\"\"\n",
    "        It checks whether any of the considered brain areas recorded in the session\n",
    "        is present in both visual or basal regions.\n",
    "        Also excludes sessions with a number of neurons < self.threshold\n",
    "        \"\"\"\n",
    "      \n",
    "        visual_and_basal_sessions = []\n",
    "        for session in data:\n",
    "        \n",
    "            visual_ok = any(area in self.visual_cortex_regions for area in session[\"brain_area\"]) \n",
    "            basal_ok = any(area in self.basal_ganglia_regions for area in session[\"brain_area\"]) \n",
    "            vis_neuron_count = len([i for i, area in enumerate(session[\"brain_area\"]) if area in self.visual_cortex_regions])\n",
    "            bg_neuron_count = len([i for i, area in enumerate(session[\"brain_area\"]) if area in self.basal_ganglia_regions])\n",
    "            min_count = np.min([vis_neuron_count, bg_neuron_count])\n",
    "\n",
    "            if visual_ok and basal_ok and min_count>self.thresh:\n",
    "                visual_and_basal_sessions.append(session)\n",
    "\n",
    "        return visual_and_basal_sessions  \n",
    "        ## a list of actual sessions (with all datas) which contains both, and the number of neurons is larger than the threshold...\n",
    "\n",
    "    def get_go_trials(\n",
    "        self,\n",
    "        data: list\n",
    "    ) -> list:\n",
    "        \"\"\"\n",
    "        Within sessions, excludes no-go trials.\n",
    "        \"\"\"\n",
    "        sessions_with_go_trials = []\n",
    "        for session in data:\n",
    "            go_trial_indices = [i for i, response in enumerate(session[\"response\"]) if response != 0]\n",
    "            if go_trial_indices:\n",
    "                filtered_session = {} # sessions are dicts\n",
    "                for key, value in session.items():\n",
    "                    # Include all the keys of the session dictionary\n",
    "                    if isinstance(value, np.ndarray) and value.ndim > 1 and value.shape[1] == len(session[\"response\"]): # like \"spks\" is multidimensional\n",
    "                        filtered_session[key] = value[:, go_trial_indices]\n",
    "                    elif isinstance(value, np.ndarray) and len(value) == len(session[\"response\"]): # like \"response is 1 dimensional\"\n",
    "                        filtered_session[key] = value[go_trial_indices]\n",
    "                    else: # like \"mouse_name\" is a single value\n",
    "                        filtered_session[key] = value\n",
    "                sessions_with_go_trials.append(filtered_session)\n",
    "        \n",
    "        return sessions_with_go_trials  \n",
    "\n",
    "    def find_min_dimensions(\n",
    "        self,\n",
    "        data: list\n",
    "    ) -> tuple:\n",
    "        min_trials = float('inf')\n",
    "        min_neurons = float('inf')\n",
    "        \"\"\"\n",
    "        Helper function to establish \"\"\"        \n",
    "        \n",
    "        for session in data:\n",
    "            spikes = session[\"spks\"]  ## [neurons_number, trial_Indexes, time]\n",
    "            min_trials = min(min_trials, spikes.shape[1])\n",
    "            # min_neurons = min(min_neurons, spikes.shape[0])\n",
    "        \n",
    "        return min_trials, min_neurons\n",
    "\n",
    "    def get_truncated_spikes(\n",
    "        self,\n",
    "        session: dict,\n",
    "        regions: list\n",
    "    ) -> torch.tensor:\n",
    "\n",
    "        neuron_indices = [i for i, area in enumerate(session[\"brain_area\"]) if area in regions]\n",
    "        spikes = session[\"spks\"][neuron_indices, :, :]  ## containing spks from visual/basal areas, e.g 17\n",
    "        spikes = spikes[:self.thresh, :, :] \n",
    "        spikes = spikes[:, :self.min_trials, :]\n",
    "\n",
    "        spikes = torch.tensor(spikes)\n",
    "        spikes = torch.permute(spikes, (2,0,1))  ## time, neuron_index, trial_index\n",
    "\n",
    "        # 24/07/15 -> Dario: should we leave the conversion to tensor in the __getitem__ method?\n",
    "\n",
    "        return spikes\n",
    "    \n",
    "    def sanity_check(self):\n",
    "        for idx, session in enumerate(self.data):\n",
    "            spikes_visual = self.get_truncated_spikes(session, self.visual_cortex_regions)\n",
    "            spikes_basal = self.get_truncated_spikes(session, self.basal_ganglia_regions)\n",
    "\n",
    "            assert spikes_visual.shape[1] == self.min_trials, f\"Session idx visual cortex trials mismatch.\"\n",
    "            assert spikes_visual.shape[2] == self.min_neurons, f\"Session idx visual cortex neurons mismatch.\"\n",
    "            assert spikes_basal.shape[1] == self.min_trials, f\"Session idx basal ganglia trials mismatch.\"\n",
    "            assert spikes_basal.shape[2] == self.min_neurons, f\"Session idx basal ganglia neurons mismatch.\"\n",
    "    print(\"Sanity check clear\")\n",
    "\n",
    "thresh=50\n",
    "all_data = CustomDataset(alldat)\n",
    "x = all_data[0]\n",
    "\n",
    "print(\"We have\", len(all_data), \"number of sessions.\")\n",
    "print(\"Each of the session has\", all_data[0][0].shape[2], \"trials, and\", all_data[0][0].shape[1], \"neurons.\")\n",
    "print(\"We have\", 123*6, \"input basal spike matrx.\")"
   ],
   "id": "62accc51c5d0d330",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check clear\n",
      "Minimum number of trials: 123\n",
      "Minimum number of neurons: 50\n",
      "We have 6 number of sessions.\n",
      "Each of the session has 123 trials, and 50 neurons.\n",
      "We have 738 input basal spike matrx.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T02:26:34.415613Z",
     "start_time": "2024-07-22T02:26:34.399825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# a = (all_data[0][0], all_data[0][1])  # Initialize 'a' with the first tuple's tensors.\n",
    "\n",
    "visual_merged = all_data[0]\n",
    "\n",
    "for i, session in enumerate(all_data):  # Start from the second element.\n",
    "    # print(i)\n",
    "    if i == 0: continue\n",
    "    visual_merged = (torch.cat((visual_merged[0], session[0]), dim=2),  # Concatenate along dim=2 for the first tensor.\n",
    "         torch.cat((visual_merged[1], session[1])))         # Simply concatenate along the last dimension for the second tensor.\n",
    "\n",
    "print(visual_merged[0].shape, visual_merged[1].shape)"
   ],
   "id": "60b409c9c1e3ef01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 50, 738]) torch.Size([738])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T02:26:36.667152Z",
     "start_time": "2024-07-22T02:26:36.654545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        # self.criterion = nn.CrossEntropyLoss()\n",
    "        # self.learning_rate = 0.001\n",
    "        # self.optimizer = torch.optim.Adam(self.model.parameters(), lr = self.learning_rate)\n",
    "        # First define the layers.\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=2) \n",
    "        # Input 1 channel, output 6 channels, referring to the high level projection to the cortex.\n",
    "        # self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
    "        self.fc1 = nn.Linear(50 * 250 * 6, 2) # projecting onto the action space, which only contains left (-1) or right (1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input basal activity (250, 50) as (batch_size, time, neurons)\n",
    "        \"\"\"\n",
    "        # Conv layer 1.\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Fully connected layer 1.\n",
    "        x = x.view(-1, 250 * 50 * 6)  # You have to first flatten the output from the previous convolution layer.\n",
    "        x = self.fc1(x)\n",
    "        # x = torch.nn.functional.softmax(x, dim=1)\n",
    "        # Cross Entropy already contains softmax computation so we dont need that here anymore! \n",
    "        return x  ## Returning the logits\n",
    "\n",
    "motion_decoder = decoder()\n",
    "motion_decoder.load_state_dict(torch.load('/home/wyd716/PycharmProjects/NMA_DL_NeuronPrediction/Motion_decoder_AllSession_Merged.pt'))\n",
    "motion_decoder.requires_grad_ = False\n",
    "motion_decoder.eval()"
   ],
   "id": "bd4a93f27479400e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decoder(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (fc1): Linear(in_features=75000, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T02:29:06.394634Z",
     "start_time": "2024-07-22T02:29:06.380318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Evo_encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, motion_decoder):\n",
    "        \n",
    "        super().__init__()\n",
    "        # self.criterion = nn.CrossEntropyLoss()\n",
    "        # self.learning_rate = 0.001\n",
    "        # self.optimizer = torch.optim.Adam(self.model.parameters(), lr = self.learning_rate)\n",
    "        # First define the layers.\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=2) \n",
    "        self.vtom = nn.Conv2d(6, 6, kernel_size=5, padding=2) # The projection from Visual Cortex to Motor cortex\n",
    "        self.vtohtm = nn.Conv2d(6, 2, kernel_size=5, padding=2) # The projection to the hypothalamus \n",
    "        self.conv2 = nn.Conv2d(8, 1, kernel_size=5, padding=2) # Combining the mortor cortex and hypothalamus\n",
    "        self.MotionDecoder = motion_decoder # projecting onto the action space, which only contains left (-1) or right (1)\n",
    "        motion_decoder.training = False\n",
    "        motion_decoder.conv1.bias.requires_grad = False\n",
    "        motion_decoder.conv1.weight.requires_grad = False\n",
    "        motion_decoder.fc1.bias.requires_grad = False\n",
    "        motion_decoder.fc1.weight.requires_grad = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input basal activity (250, 50) as (batch_size, time, neurons)\n",
    "        \"\"\"\n",
    "        # Conv layer 1.\n",
    "        x = F.relu(self.conv1(x)) # The shape of this output is (1, 6, 250, 50)\n",
    "        m = F.relu(self.vtom(x))\n",
    "        htm = F.relu(self.vtohtm(x))\n",
    "        # Fully connected layer 1.\n",
    "        x = torch.cat((m, htm), dim=1)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # x = self.MotionDecoder(x) # The input here need to be (250,50)\n",
    "        # x = torch.nn.functional.softmax(x, dim=1)\n",
    "        # Cross Entropy already contains softmax computation so we dont need that here anymore! \n",
    "        return x\n",
    "    \n",
    "Evo_encoder = Evo_encoder(motion_decoder)\n",
    "Evo_encoder.load_state_dict(torch.load('/home/wyd716/PycharmProjects/NMA_DL_NeuronPrediction/Evo_encoder.pt'))\n",
    "Evo_encoder.eval()"
   ],
   "id": "d2f7d6f9a90855bf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evo_encoder(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (vtom): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (vtohtm): Conv2d(6, 2, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv2): Conv2d(8, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (MotionDecoder): decoder(\n",
       "    (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (fc1): Linear(in_features=75000, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        # self.criterion = nn.CrossEntropyLoss()\n",
    "        # self.learning_rate = 0.001\n",
    "        # self.optimizer = torch.optim.Adam(self.model.parameters(), lr = self.learning_rate)\n",
    "        # First define the layers.\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=2) \n",
    "        self.vtom = nn.Conv2d(6, 6, kernel_size=5, padding=2) # The projection from Visual Cortex to Motor cortex\n",
    "        self.vtohtm = nn.Conv2d(6, 2, kernel_size=5, padding=2) # The projection to the hypothalamus \n",
    "        self.conv2 = nn.Conv2d(8, 1, kernel_size=5, padding=2) # Combining the mortor cortex and hypothalamus\n",
    "        self.fc1 = nn.Linear(250*50, 2)\n",
    "        self.fc1.training = False\n",
    "        self.fc1.bias.requires_grad_ = False\n",
    "        self.fc1.weight.requires_grad = False\n",
    "        # self.MotionDecoder = motion_decoder # projecting onto the action space, which only contains left (-1) or right (1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input basal activity (250, 50) as (batch_size, time, neurons)\n",
    "        \"\"\"\n",
    "        # Conv layer 1.\n",
    "        x = F.relu(self.conv1(x)) # The shape of this output is (1, 6, 250, 50)\n",
    "        m = F.relu(self.vtom(x))\n",
    "        htm = F.relu(self.vtohtm(x))\n",
    "        # Fully connected layer 1.\n",
    "        x = torch.cat((m, htm), dim=1)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(250, 50)\n",
    "        # x = self.fc1(x) # The input here need to be (250,50)\n",
    "        # x = torch.nn.functional.softmax(x, dim=1)\n",
    "        # Cross Entropy already contains softmax computation so we dont need that here anymore! \n",
    "        return x\n",
    "    \n",
    "encoder = encoder()\n",
    "encoder.load_state_dict(torch.load('/home/wyd716/PycharmProjects/NMA_DL_NeuronPrediction/encoder.pt'))\n",
    "encoder.eval()"
   ],
   "id": "dc4867480872f6da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T02:51:20.058247Z",
     "start_time": "2024-07-22T02:51:19.577341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "evo_pred = torch.empty((250, 50, 738), dtype=torch.int8)\n",
    "for i in range(visual_merged[0].shape[2]):\n",
    "    x = visual_merged[0][:, :, i]\n",
    "    x = x.unsqueeze(0).unsqueeze(0)\n",
    "    x = x.float()\n",
    "    x = Evo_encoder.forward(x)\n",
    "    x = x.squeeze(0,1)\n",
    "    evo_pred[:, :, i] = x\n",
    "evo_pred_merged = (evo_pred, visual_merged[1])"
   ],
   "id": "220c912bd1cc1c15",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T02:57:18.996888Z",
     "start_time": "2024-07-22T02:57:18.515869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "control_pred = torch.empty((250, 50, 738), dtype=torch.int8)\n",
    "for i in range(visual_merged[0].shape[2]):\n",
    "    x = visual_merged[0][:, :, i]\n",
    "    x = x.unsqueeze(0).unsqueeze(0)\n",
    "    x = x.float()\n",
    "    x = encoder.forward(x)\n",
    "    x = x.view((250, 50))\n",
    "    x = x.squeeze(0,1)\n",
    "    control_pred[:, :, i] = x\n",
    "control_pred_merged = (control_pred, visual_merged[1])"
   ],
   "id": "a0c113cb530681c2",
   "outputs": [],
   "execution_count": 62
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
