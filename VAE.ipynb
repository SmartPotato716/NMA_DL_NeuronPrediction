{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T01:28:35.999151Z",
     "start_time": "2024-07-24T01:28:33.258315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# import torchvision.transforms as transforms\n",
    "# import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T01:28:36.006539Z",
     "start_time": "2024-07-24T01:28:36.001054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_device():\n",
    "    \"\"\"\n",
    "    Set the device. CUDA if available, otherwise CPU.\n",
    "    \n",
    "    Args: None\n",
    "    \n",
    "    Returns: Nothing   \n",
    "    \"\"\"\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device != \"cuda\":\n",
    "        print(\"This notebook will be running on CPU!\")\n",
    "    else: \n",
    "        print(\"You have CUDA set up! Woohoo!!!\")\n",
    "        \n",
    "    return device"
   ],
   "id": "f7cc561c64315925",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T01:28:36.052278Z",
     "start_time": "2024-07-24T01:28:36.008550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# SEED = 2023\n",
    "# set_seed(seed=SEED)\n",
    "DEVICE = set_device()"
   ],
   "id": "126d4a03e49a8753",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook will be running on CPU!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T01:28:59.315944Z",
     "start_time": "2024-07-24T01:28:36.055285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, requests\n",
    "\n",
    "fname = []\n",
    "for j in range(3):\n",
    "  fname.append('steinmetz_part%d.npz'%j)\n",
    "url = [\"https://osf.io/agvxh/download\"]\n",
    "url.append(\"https://osf.io/uv3mw/download\")\n",
    "url.append(\"https://osf.io/ehmw2/download\")\n",
    "\n",
    "for j in range(len(url)):\n",
    "  if not os.path.isfile(fname[j]):\n",
    "    try:\n",
    "      r = requests.get(url[j])\n",
    "    except requests.ConnectionError:\n",
    "      print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "      if r.status_code != requests.codes.ok:\n",
    "        print(\"!!! Failed to download data !!!\")\n",
    "      else:\n",
    "        with open(fname[j], \"wb\") as fid:\n",
    "          fid.write(r.content)\n",
    "          \n",
    "alldat = np.array([])\n",
    "for j in range(len(fname)):\n",
    "  alldat = np.hstack((alldat,\n",
    "                      np.load('steinmetz_part%d.npz'%j,\n",
    "                              allow_pickle=True)['dat']))"
   ],
   "id": "285b69b6a7aeed36",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T01:28:59.699331Z",
     "start_time": "2024-07-24T01:28:59.316944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "basal_ganglia_regions = [\"ACB\", \"CP\", \"GPe\", \"LS\", \"LSc\", \"LSr\", \"MS\", \"OT\", \"SNr\", \"SI\"]\n",
    "visual_cortex_regions = [\"VISa\", \"VISam\", \"VISl\", \"VISp\", \"VISpm\", \"VISrl\"]\n",
    "\n",
    "total_vc_neurons=0\n",
    "total_bg_neurons=0\n",
    "\n",
    "sessions_to_use = []  ##Session indices\n",
    "session_lengths = []  ##The number of trials in each session\n",
    "input_visual_tensor_list = []\n",
    "label_basal_tensor_list = []\n",
    "\n",
    "for session in range(len(alldat)):\n",
    "  num_vis_neurons = 0\n",
    "  num_basal_neurons = 0\n",
    "  vis_neuron_indices = []\n",
    "  basal_neuron_indices = []\n",
    "  for neuron_idx, area in enumerate(alldat[session]['brain_area']):\n",
    "    if area in visual_cortex_regions:  ## Viusal areas\n",
    "      num_vis_neurons += 1\n",
    "      vis_neuron_indices.append(neuron_idx)\n",
    "    if area in basal_ganglia_regions:  ## Basal Ganglia\n",
    "      num_basal_neurons += 1\n",
    "      basal_neuron_indices.append(neuron_idx)\n",
    "          \n",
    "  trial_num = alldat[session]['spks'].shape[1]   \n",
    "  input_visual_recordings = np.empty((num_vis_neurons, trial_num, 250))\n",
    "  label_basal_recordings = np.empty((num_basal_neurons, trial_num, 250))\n",
    "  \n",
    "  \n",
    "  if num_vis_neurons*num_basal_neurons != 0:\n",
    "    sessions_to_use.append(session)\n",
    "    print(\"session \" ,session, \"has both visual and basal!\")\n",
    "    print('The name of the mouse: ' + alldat[session]['mouse_name'])\n",
    "    print('The session time' + alldat[session]['date_exp'])\n",
    "    print(alldat[session]['spks'].shape)\n",
    "\n",
    "    print('The number of neurons being recorded:', len(alldat[session]['spks']))\n",
    "    print('Number of visual neurons: ', num_vis_neurons)\n",
    "    # print('Indices of visual neurons: ', vis_neuron_indices)\n",
    "    \n",
    "    ## To extract the data from alldat[current_session]\n",
    "    for i, v in enumerate(vis_neuron_indices): \n",
    "      input_visual_recordings[i, :, :] = alldat[session]['spks'][v, :, :]\n",
    "    print(\"reorganized visual activities for input: \", input_visual_recordings.shape)\n",
    "    print('Number of basal neurons: ', num_basal_neurons)\n",
    "    # print('Indices of basal neurons: ', basal_neuron_indices)\n",
    "    for i, g in enumerate(basal_neuron_indices):\n",
    "      label_basal_recordings[i, :, :] = alldat[session]['spks'][g, :, :]\n",
    "    print(\"reorganized basal activities for label\", label_basal_recordings.shape)\n",
    "    print('----------------------------------------------------')\n",
    "    \n",
    "    visual_input_tensor = torch.from_numpy(input_visual_recordings).reshape(-1, 250)\n",
    "    input_visual_tensor_list.append(visual_input_tensor)\n",
    "    basal_label_tensor = torch.from_numpy(label_basal_recordings).reshape(-1, 250)\n",
    "    label_basal_tensor_list.append(basal_label_tensor)\n",
    "\n",
    "# input = torch.cat(input_visual_tensor_list, axis = 0)\n",
    "# label = torch.cat(label_basal_tensor_list, axis = 0)\n",
    "# !!!!!! I realized that the firing rate across different sessions have huge variations, so it may not be a good idea to combine them together!!!!!\\\n",
    "\n",
    "print(\"input tensor list: We have\", len(input_visual_tensor_list), \"and their shapes are \", [input_visual_tensor_list[i].shape for i in range(len(input_visual_tensor_list))])\n",
    "print(\"label tensor list: We have\", len(label_basal_tensor_list), \"and their shapes are\", [label_basal_tensor_list[i].shape for i in range(len(label_basal_tensor_list))])\n"
   ],
   "id": "104976d78b284d7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session  0 has both visual and basal!\n",
      "The name of the mouse: Cori\n",
      "The session time2016-12-14\n",
      "(734, 214, 250)\n",
      "The number of neurons being recorded: 734\n",
      "Number of visual neurons:  178\n",
      "reorganized visual activities for input:  (178, 214, 250)\n",
      "Number of basal neurons:  139\n",
      "reorganized basal activities for label (139, 214, 250)\n",
      "----------------------------------------------------\n",
      "session  3 has both visual and basal!\n",
      "The name of the mouse: Forssmann\n",
      "The session time2017-11-01\n",
      "(1769, 249, 250)\n",
      "The number of neurons being recorded: 1769\n",
      "Number of visual neurons:  120\n",
      "reorganized visual activities for input:  (120, 249, 250)\n",
      "Number of basal neurons:  435\n",
      "reorganized basal activities for label (435, 249, 250)\n",
      "----------------------------------------------------\n",
      "session  7 has both visual and basal!\n",
      "The name of the mouse: Hench\n",
      "The session time2017-06-15\n",
      "(1156, 250, 250)\n",
      "The number of neurons being recorded: 1156\n",
      "Number of visual neurons:  111\n",
      "reorganized visual activities for input:  (111, 250, 250)\n",
      "Number of basal neurons:  3\n",
      "reorganized basal activities for label (3, 250, 250)\n",
      "----------------------------------------------------\n",
      "session  8 has both visual and basal!\n",
      "The name of the mouse: Hench\n",
      "The session time2017-06-16\n",
      "(788, 372, 250)\n",
      "The number of neurons being recorded: 788\n",
      "Number of visual neurons:  142\n",
      "reorganized visual activities for input:  (142, 372, 250)\n",
      "Number of basal neurons:  33\n",
      "reorganized basal activities for label (33, 372, 250)\n",
      "----------------------------------------------------\n",
      "session  9 has both visual and basal!\n",
      "The name of the mouse: Hench\n",
      "The session time2017-06-17\n",
      "(1172, 447, 250)\n",
      "The number of neurons being recorded: 1172\n",
      "Number of visual neurons:  340\n",
      "reorganized visual activities for input:  (340, 447, 250)\n",
      "Number of basal neurons:  63\n",
      "reorganized basal activities for label (63, 447, 250)\n",
      "----------------------------------------------------\n",
      "session  12 has both visual and basal!\n",
      "The name of the mouse: Lederberg\n",
      "The session time2017-12-06\n",
      "(983, 300, 250)\n",
      "The number of neurons being recorded: 983\n",
      "Number of visual neurons:  34\n",
      "reorganized visual activities for input:  (34, 300, 250)\n",
      "Number of basal neurons:  23\n",
      "reorganized basal activities for label (23, 300, 250)\n",
      "----------------------------------------------------\n",
      "session  21 has both visual and basal!\n",
      "The name of the mouse: Muller\n",
      "The session time2017-01-07\n",
      "(646, 444, 250)\n",
      "The number of neurons being recorded: 646\n",
      "Number of visual neurons:  133\n",
      "reorganized visual activities for input:  (133, 444, 250)\n",
      "Number of basal neurons:  92\n",
      "reorganized basal activities for label (92, 444, 250)\n",
      "----------------------------------------------------\n",
      "session  26 has both visual and basal!\n",
      "The name of the mouse: Radnitz\n",
      "The session time2017-01-10\n",
      "(563, 253, 250)\n",
      "The number of neurons being recorded: 563\n",
      "Number of visual neurons:  103\n",
      "reorganized visual activities for input:  (103, 253, 250)\n",
      "Number of basal neurons:  90\n",
      "reorganized basal activities for label (90, 253, 250)\n",
      "----------------------------------------------------\n",
      "session  29 has both visual and basal!\n",
      "The name of the mouse: Richards\n",
      "The session time2017-10-29\n",
      "(942, 143, 250)\n",
      "The number of neurons being recorded: 942\n",
      "Number of visual neurons:  19\n",
      "reorganized visual activities for input:  (19, 143, 250)\n",
      "Number of basal neurons:  47\n",
      "reorganized basal activities for label (47, 143, 250)\n",
      "----------------------------------------------------\n",
      "session  34 has both visual and basal!\n",
      "The name of the mouse: Tatum\n",
      "The session time2017-12-06\n",
      "(795, 311, 250)\n",
      "The number of neurons being recorded: 795\n",
      "Number of visual neurons:  75\n",
      "reorganized visual activities for input:  (75, 311, 250)\n",
      "Number of basal neurons:  219\n",
      "reorganized basal activities for label (219, 311, 250)\n",
      "----------------------------------------------------\n",
      "input tensor list: We have 10 and their shapes are  [torch.Size([38092, 250]), torch.Size([29880, 250]), torch.Size([27750, 250]), torch.Size([52824, 250]), torch.Size([151980, 250]), torch.Size([10200, 250]), torch.Size([59052, 250]), torch.Size([26059, 250]), torch.Size([2717, 250]), torch.Size([23325, 250])]\n",
      "label tensor list: We have 10 and their shapes are [torch.Size([29746, 250]), torch.Size([108315, 250]), torch.Size([750, 250]), torch.Size([12276, 250]), torch.Size([28161, 250]), torch.Size([6900, 250]), torch.Size([40848, 250]), torch.Size([22770, 250]), torch.Size([6721, 250]), torch.Size([68109, 250])]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`alldat` contains 39 sessions from 10 mice, data from Steinmetz et al, 2019. Time bins for all measurements are 10ms, starting 500ms before stimulus onset. The mouse had to determine which side has the highest contrast. For each `dat = alldat[k]`, you have the fields below. For extra variables, check out the extra notebook and extra data files (lfp, waveforms and exact spike times, non-binned). \n",
    "\n",
    "* `dat['mouse_name']`: mouse name\n",
    "* `dat['date_exp']`: when a session was performed\n",
    "* `dat['spks']`: neurons by trials by time bins.    \n",
    "* `dat['brain_area']`: brain area for each neuron recorded. \n",
    "* `dat['ccf']`: Allen Institute brain atlas coordinates for each neuron. \n",
    "* `dat['ccf_axes']`: axes names for the Allen CCF. \n",
    "* `dat['contrast_right']`: contrast level for the right stimulus, which is always contralateral to the recorded brain areas.\n",
    "* `dat['contrast_left']`: contrast level for left stimulus. \n",
    "* `dat['gocue']`: when the go cue sound was played. \n",
    "* `dat['response_time']`: when the response was registered, which has to be after the go cue. The mouse can turn the wheel before the go cue (and nearly always does!), but the stimulus on the screen won't move before the go cue.  \n",
    "* `dat['response']`: which side the response was (`-1`, `0`, `1`). When the right-side stimulus had higher contrast, the correct choice was `-1`. `0` is a no go response. \n",
    "* `dat['feedback_time']`: when feedback was provided. \n",
    "* `dat['feedback_type']`: if the feedback was positive (`+1`, reward) or negative (`-1`, white noise burst).  \n",
    "* `dat['wheel']`: turning speed of the wheel that the mice uses to make a response, sampled at `10ms`. \n",
    "* `dat['pupil']`: pupil area  (noisy, because pupil is very small) + pupil horizontal and vertical position.\n",
    "* `dat['face']`: average face motion energy from a video camera. \n",
    "* `dat['licks']`: lick detections, 0 or 1.   \n",
    "* `dat['trough_to_peak']`: measures the width of the action potential waveform for each neuron. Widths `<=10` samples are \"putative fast spiking neurons\". \n",
    "* `dat['%X%_passive']`: same as above for `X` = {`spks`, `pupil`, `wheel`, `contrast_left`, `contrast_right`} but for  passive trials at the end of the recording when the mouse was no longer engaged and stopped making responses. \n",
    "* `dat['prev_reward']`: time of the feedback (reward/white noise) on the previous trial in relation to the current stimulus time. \n",
    "* `dat['reaction_time']`: ntrials by 2. First column: reaction time computed from the wheel movement as the first sample above `5` ticks/10ms bin. Second column: direction of the wheel movement (`0` = no move detected).  \n",
    "\n",
    "\n",
    "The original dataset is here: https://figshare.com/articles/dataset/Dataset_from_Steinmetz_et_al_2019/9598406"
   ],
   "id": "3cc4357dba58f0e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T01:29:00.014575Z",
     "start_time": "2024-07-24T01:28:59.701327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 2024/07/15 We set the turncation threshold to be 50 neurons\n",
    "## adding reshape methods -- please go over the code to see if modified/added \"aspects\" are consistent \n",
    "## and do what they are intended to do, the number of neurons went down (perhaps a little bit more than intended)\n",
    "## and the same for the trial number, but now they match in dimensions\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    I tried to make it tailored to our needs.\n",
    "    It only considers sessions that have both visual and basal ganglia recordings.\n",
    "    It only considers go trials.\n",
    "    When indexed, it returns basal ganglia spikes and action).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        visual_cortex_regions: list = [\"VISa\", \"VISam\", \"VISl\", \"VISp\", \"VISpm\", \"VISrl\"], # Put them as default argument so they can be easily changed if we decide to change areas\n",
    "        basal_ganglia_regions: list = [\"ACB\", \"CP\", \"GPe\", \"LS\", \"LSc\", \"LSr\", \"MS\", \"OT\", \"SNr\", \"SI\"],\n",
    "        thresh=50\n",
    "    ) -> None:\n",
    "        \n",
    "        self.visual_cortex_regions = visual_cortex_regions\n",
    "        self.basal_ganglia_regions = basal_ganglia_regions\n",
    "        self.thresh=thresh\n",
    "        self.data = self.visual_and_basal(data)\n",
    "        self.data = self.get_go_trials(self.data)\n",
    "        self.min_trials, self.min_neurons = self.find_min_dimensions(self.data) # new \n",
    "        print(f\"Minimum number of trials: {self.min_trials}\")\n",
    "        print(f\"Minimum number of neurons: {self.thresh}\")\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        session_index: int\n",
    "    ) -> torch.tensor:\n",
    "        \n",
    "        session = self.data[session_index]\n",
    "        \n",
    "        # Since we don't need the visual spks so I am commentting this out. (YD, 07/16)\n",
    "        spikes_visual = self.get_truncated_spikes(session, self.visual_cortex_regions)\n",
    "        \n",
    "        ## Some number checks by Yangdong 24/7/15\n",
    "        ## print(\"the shape of spikes_visual after truncating:\", spikes_visual.shape)\n",
    "\n",
    "        basal_spks = self.get_truncated_spikes(session, self.basal_ganglia_regions) # This will be the input data (time, trials, eurons_indicies)          \n",
    "        label_action = torch.tensor(session['response'][0:self.min_trials]) #(-1: the mouse is turning the wheel to the left, which indicates that the mouse thinks the right side has higher contrast level. )\n",
    "        \n",
    "        return spikes_visual, basal_spks\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.data)\n",
    "\n",
    "    def visual_and_basal(\n",
    "        self,\n",
    "        data: np.array\n",
    "    ) -> list:\n",
    "        \"\"\"\n",
    "        It checks whether any of the considered brain areas recorded in the session\n",
    "        is present in both visual or basal regions.\n",
    "        Also excludes sessions with a number of neurons < self.threshold\n",
    "        \"\"\"\n",
    "      \n",
    "        visual_and_basal_sessions = []\n",
    "        for session in data:\n",
    "        \n",
    "            visual_ok = any(area in self.visual_cortex_regions for area in session[\"brain_area\"]) \n",
    "            basal_ok = any(area in self.basal_ganglia_regions for area in session[\"brain_area\"]) \n",
    "            vis_neuron_count = len([i for i, area in enumerate(session[\"brain_area\"]) if area in self.visual_cortex_regions])\n",
    "            bg_neuron_count = len([i for i, area in enumerate(session[\"brain_area\"]) if area in self.basal_ganglia_regions])\n",
    "            min_count = np.min([vis_neuron_count, bg_neuron_count])\n",
    "\n",
    "            if visual_ok and basal_ok and min_count>self.thresh:\n",
    "                visual_and_basal_sessions.append(session)\n",
    "\n",
    "        return visual_and_basal_sessions  \n",
    "        ## a list of actual sessions (with all datas) which contains both, and the number of neurons is larger than the threshold...\n",
    "\n",
    "    def get_go_trials(\n",
    "        self,\n",
    "        data: list\n",
    "    ) -> list:\n",
    "        \"\"\"\n",
    "        Within sessions, excludes no-go trials.\n",
    "        \"\"\"\n",
    "        sessions_with_go_trials = []\n",
    "        for session in data:\n",
    "            go_trial_indices = [i for i, response in enumerate(session[\"response\"]) if response != 0]\n",
    "            if go_trial_indices:\n",
    "                filtered_session = {} # sessions are dicts\n",
    "                for key, value in session.items():\n",
    "                    # Include all the keys of the session dictionary\n",
    "                    if isinstance(value, np.ndarray) and value.ndim > 1 and value.shape[1] == len(session[\"response\"]): # like \"spks\" is multidimensional\n",
    "                        filtered_session[key] = value[:, go_trial_indices]\n",
    "                    elif isinstance(value, np.ndarray) and len(value) == len(session[\"response\"]): # like \"response is 1 dimensional\"\n",
    "                        filtered_session[key] = value[go_trial_indices]\n",
    "                    else: # like \"mouse_name\" is a single value\n",
    "                        filtered_session[key] = value\n",
    "                sessions_with_go_trials.append(filtered_session)\n",
    "        \n",
    "        return sessions_with_go_trials  \n",
    "\n",
    "    def find_min_dimensions(\n",
    "        self,\n",
    "        data: list\n",
    "    ) -> tuple:\n",
    "        min_trials = float('inf')\n",
    "        min_neurons = float('inf')\n",
    "        \"\"\"\n",
    "        Helper function to establish \"\"\"        \n",
    "        \n",
    "        for session in data:\n",
    "            spikes = session[\"spks\"]  ## [neurons_number, trial_Indexes, time]\n",
    "            min_trials = min(min_trials, spikes.shape[1])\n",
    "            # min_neurons = min(min_neurons, spikes.shape[0])\n",
    "        \n",
    "        return min_trials, min_neurons\n",
    "\n",
    "    def get_truncated_spikes(\n",
    "        self,\n",
    "        session: dict,\n",
    "        regions: list\n",
    "    ) -> torch.tensor:\n",
    "\n",
    "        neuron_indices = [i for i, area in enumerate(session[\"brain_area\"]) if area in regions]\n",
    "        spikes = session[\"spks\"][neuron_indices, :, :]  ## containing spks from visual/basal areas, e.g 17\n",
    "        spikes = spikes[:self.thresh, :, :] \n",
    "        spikes = spikes[:, :self.min_trials, :]\n",
    "\n",
    "        spikes = torch.tensor(spikes)\n",
    "        spikes = torch.permute(spikes, (2,0,1))  ## time, neuron_index, trial_index\n",
    "\n",
    "        # 24/07/15 -> Dario: should we leave the conversion to tensor in the __getitem__ method?\n",
    "\n",
    "        return spikes\n",
    "    \n",
    "    def sanity_check(self):\n",
    "        for idx, session in enumerate(self.data):\n",
    "            spikes_visual = self.get_truncated_spikes(session, self.visual_cortex_regions)\n",
    "            spikes_basal = self.get_truncated_spikes(session, self.basal_ganglia_regions)\n",
    "\n",
    "            assert spikes_visual.shape[1] == self.min_trials, f\"Session idx visual cortex trials mismatch.\"\n",
    "            assert spikes_visual.shape[2] == self.min_neurons, f\"Session idx visual cortex neurons mismatch.\"\n",
    "            assert spikes_basal.shape[1] == self.min_trials, f\"Session idx basal ganglia trials mismatch.\"\n",
    "            assert spikes_basal.shape[2] == self.min_neurons, f\"Session idx basal ganglia neurons mismatch.\"\n",
    "    print(\"Sanity check clear\")\n",
    "\n",
    "thresh=50\n",
    "all_data = CustomDataset(alldat)\n",
    "x = all_data[0]\n",
    "\n",
    "print(\"We have\", len(all_data), \"number of sessions.\")\n",
    "print(\"Each of the session has\", all_data[0][0].shape[2], \"trials, and\", all_data[0][0].shape[1], \"neurons.\")\n",
    "print(\"We have\", 123*6, \"input basal spike matrx.\")\n"
   ],
   "id": "82bf38888626145f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check clear\n",
      "Minimum number of trials: 123\n",
      "Minimum number of neurons: 50\n",
      "We have 6 number of sessions.\n",
      "Each of the session has 123 trials, and 50 neurons.\n",
      "We have 738 input basal spike matrx.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T01:29:00.115367Z",
     "start_time": "2024-07-24T01:29:00.016591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# a = (all_data[0][0], all_data[0][1])  # Initialize 'a' with the first tuple's tensors.\n",
    "\n",
    "merged = all_data[0]\n",
    "\n",
    "for i, session in enumerate(all_data):  # Start from the second element.\n",
    "    # print(i)\n",
    "    if i == 0: continue\n",
    "    merged = (torch.cat((merged[0], session[0]), dim=2),  # Concatenate along dim=2 for the first tensor.\n",
    "         torch.cat((merged[1], session[1]), dim=2))         # Simply concatenate along the last dimension for the second tensor.\n",
    "\n",
    "print(merged[0].shape, merged[1].shape)"
   ],
   "id": "753dcd9c0771593b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 50, 738]) torch.Size([250, 50, 738])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T01:29:00.127176Z",
     "start_time": "2024-07-24T01:29:00.116379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import SubsetRandomSampler, DataLoader, Dataset\n",
    "\n",
    "class Session_New(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_spikes = self.data[0][:, :, idx] # input_spikes has the shape (time, neuron_index)\n",
    "        label_spikes = self.data[1][:, :, idx] # has the action tensor (1) or (-1).\n",
    "        return input_spikes, label_spikes\n",
    "\n",
    "def split(dataset, validation_split=0.2, test_split = 0.2, shuffle_trials=True, batch_size=20):\n",
    "    num_trials = merged[0].shape[2]\n",
    "    indices = list(range(num_trials))\n",
    "    validation_split_index = int(np.floor(validation_split * num_trials))\n",
    "    test_split_index = int(np.floor(test_split * num_trials)) + validation_split_index\n",
    "    if shuffle_trials:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    train_indices = indices[test_split_index:]\n",
    "    test_indices =  indices[validation_split_index:test_split_index],\n",
    "    val_indices = indices[:validation_split_index]\n",
    "    \n",
    "    # Creating PT data samplers and loaders\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "    test_sampler = SubsetRandomSampler(test_indices)\n",
    "    \n",
    "    dataset = Session_New(dataset)\n",
    "    \n",
    "    train_loader = DataLoader(dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              sampler = train_sampler\n",
    "                              )\n",
    "    valid_loader = DataLoader(dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              sampler = valid_sampler\n",
    "                              )\n",
    "    test_loader = DataLoader(dataset, \n",
    "                             batch_size=batch_size, \n",
    "                             sampler = test_sampler)\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "train_loader, valid_loader, test_loader = split(merged)\n",
    "\n",
    "# for batch_index, (batch_neural_recordings, batch_actions) in enumerate(train_loader):\n",
    "#     print(f\"Batch index: {batch_index}:\")\n",
    "#     print(f\"batch_neural_recordings shape: {batch_neural_recordings.shape}\")\n",
    "#     print(f\"Batch_actions: {batch_actions}\")\n",
    "#     \n",
    "print(\"-------------------------------------------------------------------\")\n",
    "# \n",
    "# for batch_index, (batch_neural_recordings, batch_actions) in enumerate(valid_loader):\n",
    "#     print(f\"Batch index: {batch_index}:\")\n",
    "#     print(f\"batch_neural_recordings shape: {batch_neural_recordings.shape}\")\n",
    "#     print(f\"Batch_actions: {batch_actions}\")"
   ],
   "id": "7faf294222f2877a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T01:29:01.069707Z",
     "start_time": "2024-07-24T01:29:00.128185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "def z_score(X):\n",
    "    # X: ndarray, shape (n_neurons, n_timebins)\n",
    "    ss = StandardScaler(with_mean=True, with_std=True)\n",
    "    Xz = ss.fit_transform(X.T).T\n",
    "    return Xz\n",
    "\n",
    "def normalization(data):\n",
    "    '''\n",
    "    \n",
    "    We normalize the data before feeding it into the forward function. It is important to keep other attributes from data loader hence the following codes with seemingly weird logic.\n",
    "    \n",
    "    :param data: input tensor (1, 1, 250, 50)\n",
    "    :return: the 2 and 3rd dimension of data tensor got normalized\n",
    "    '''\n",
    " \n",
    "    x = data[0, 0, :, :]\n",
    "    x = torch.transpose(x, 0, 1).numpy()\n",
    "    x = z_score(x)\n",
    "    x = x.T\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    data[0, 0, :, :] = x\n",
    "    \n",
    "    return data\n",
    "    "
   ],
   "id": "9fbb1626050d93b0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T01:29:01.077718Z",
     "start_time": "2024-07-24T01:29:01.071718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BiasLayer(nn.Module):\n",
    "  \"\"\"\n",
    "  Bias Layer\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, shape):\n",
    "    \"\"\"\n",
    "    Initialise parameters of bias layer\n",
    "\n",
    "    Args:\n",
    "      shape: tuple\n",
    "        Requisite shape of bias layer\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    super(BiasLayer, self).__init__()\n",
    "    init_bias = torch.zeros(shape)\n",
    "    self.bias = nn.Parameter(init_bias, requires_grad=True)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Forward pass\n",
    "\n",
    "    Args:\n",
    "      x: torch.tensor\n",
    "        Input features\n",
    "\n",
    "    Returns:\n",
    "      Output of bias layer\n",
    "    \"\"\"\n",
    "    return x + self.bias"
   ],
   "id": "c1a197e8ed3981cc",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T01:29:01.121677Z",
     "start_time": "2024-07-24T01:29:01.078717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cout(x, layer):\n",
    "  \"\"\"\n",
    "  Unnecessarily complicated but complete way to\n",
    "  calculate the output depth, height\n",
    "  and width size for a Conv2D layer\n",
    "\n",
    "  Args:\n",
    "    x: tuple\n",
    "      Input size (depth, height, width)\n",
    "    layer: nn.Conv2d\n",
    "      The Conv2D layer\n",
    "\n",
    "  Returns:\n",
    "    Tuple of out-depth/out-height and out-width\n",
    "    Output shape as given in [Ref]\n",
    "    Ref:\n",
    "    https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "  \"\"\"\n",
    "  assert isinstance(layer, nn.Conv2d)\n",
    "  p = layer.padding if isinstance(layer.padding, tuple) else (layer.padding,)\n",
    "  k = layer.kernel_size if isinstance(layer.kernel_size, tuple) else (layer.kernel_size,)\n",
    "  d = layer.dilation if isinstance(layer.dilation, tuple) else (layer.dilation,)\n",
    "  s = layer.stride if isinstance(layer.stride, tuple) else (layer.stride,)\n",
    "  in_depth, in_height, in_width = x\n",
    "  out_depth = layer.out_channels\n",
    "  out_height = 1 + (in_height + 2 * p[0] - (k[0] - 1) * d[0] - 1) // s[0]\n",
    "  out_width = 1 + (in_width + 2 * p[-1] - (k[-1] - 1) * d[-1] - 1) // s[-1]\n",
    "  return (out_depth, out_height, out_width)"
   ],
   "id": "c1710ae22970ac3c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T01:29:01.326886Z",
     "start_time": "2024-07-24T01:29:01.123656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ConvAutoEncoder(nn.Module):\n",
    "  \"\"\"\n",
    "  A Convolutional AutoEncoder\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, x_dim, h_dim, n_filters=32, filter_size=5, batch_n = 20):\n",
    "    \"\"\"\n",
    "    Initialize parameters of ConvAutoEncoder\n",
    "\n",
    "    Args:\n",
    "      x_dim: tuple\n",
    "        Input dimensions (channels, height, widths)\n",
    "      h_dim: int\n",
    "        Hidden dimension, bottleneck dimension, K\n",
    "      n_filters: int\n",
    "        Number of filters (number of output channels)\n",
    "      filter_size: int\n",
    "        Kernel size\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    channels, height, widths = x_dim\n",
    "\n",
    "    # Encoder input bias layer\n",
    "    self.enc_bias = BiasLayer(x_dim)\n",
    "\n",
    "    # First encoder conv2d layer\n",
    "    self.enc_conv_1 = nn.Conv2d(channels, n_filters, filter_size)\n",
    "\n",
    "    # Output shape of the first encoder conv2d layer given x_dim input\n",
    "    conv_1_shape = cout(x_dim, self.enc_conv_1)\n",
    "\n",
    "    # Second encoder conv2d layer\n",
    "    self.enc_conv_2 = nn.Conv2d(n_filters, n_filters, filter_size)\n",
    "\n",
    "    # Output shape of the second encoder conv2d layer given conv_1_shape input\n",
    "    conv_2_shape = cout(conv_1_shape, self.enc_conv_2)\n",
    "\n",
    "    # The bottleneck is a dense layer, therefore we need a flattenning layer\n",
    "    self.enc_flatten = nn.Flatten()\n",
    "\n",
    "    # Conv output shape is (depth, height, width), so the flatten size is:\n",
    "    flat_after_conv = conv_2_shape[0] * conv_2_shape[1] * conv_2_shape[2]\n",
    "\n",
    "    # Encoder Linear layer\n",
    "    self.enc_lin = nn.Linear(flat_after_conv, h_dim)\n",
    "\n",
    "    # Decoder Linear layer\n",
    "    self.dec_lin = nn.Linear(h_dim, flat_after_conv)\n",
    "\n",
    "    # Unflatten data to (depth, height, width) shape\n",
    "    self.dec_unflatten = nn.Unflatten(dim=-1, unflattened_size=conv_2_shape)\n",
    "\n",
    "    # First \"deconvolution\" layer\n",
    "    self.dec_deconv_1 = nn.ConvTranspose2d(n_filters, n_filters, filter_size)\n",
    "\n",
    "    # Second \"deconvolution\" layer\n",
    "    self.dec_deconv_2 = nn.ConvTranspose2d(n_filters, channels, filter_size)\n",
    "\n",
    "    # Decoder output bias layer\n",
    "    self.dec_bias = BiasLayer(x_dim)\n",
    "\n",
    "  def encode(self, x):\n",
    "    \"\"\"\n",
    "    Encoder\n",
    "\n",
    "    Args:\n",
    "      x: torch.tensor\n",
    "        Input features\n",
    "\n",
    "    Returns:\n",
    "      h: torch.tensor\n",
    "        Encoded output\n",
    "    \"\"\"\n",
    "    s = self.enc_bias(x)\n",
    "    s = s.unsqueeze(1)\n",
    "    s = F.relu(self.enc_conv_1(s))\n",
    "    s = F.relu(self.enc_conv_2(s))\n",
    "    s = self.enc_flatten(s)\n",
    "    h = self.enc_lin(s)\n",
    "    return h\n",
    "\n",
    "  def decode(self, h):\n",
    "    \"\"\"\n",
    "    Decoder\n",
    "\n",
    "    Args:\n",
    "      h: torch.tensor\n",
    "        Encoded output\n",
    "\n",
    "    Returns:\n",
    "      x_prime: torch.tensor\n",
    "        Decoded output\n",
    "    \"\"\"\n",
    "    s = F.relu(self.dec_lin(h))\n",
    "    s = self.dec_unflatten(s)\n",
    "    s = F.relu(self.dec_deconv_1(s))\n",
    "    s = self.dec_deconv_2(s)\n",
    "    x_prime = self.dec_bias(s)\n",
    "    x_prime = x_prime.squeeze(1)\n",
    "    return x_prime\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Forward pass\n",
    "\n",
    "    Args:\n",
    "      x: torch.tensor\n",
    "        Input features\n",
    "\n",
    "    Returns:\n",
    "      Decoded output\n",
    "    \"\"\"\n",
    "    return self.decode(self.encode(x))\n",
    "\n",
    "data_shape = (1, 250, 50)\n",
    "K = 50\n",
    "trained_conv_AE = ConvAutoEncoder(data_shape, K)\n",
    "# assert trained_conv_AE.encode(train_set[0][0].unsqueeze(0)).numel() == K, \"Encoder output size should be K!\"\n",
    "# \n",
    "# plot_conv_ae(lin_losses, conv_losses)"
   ],
   "id": "fd814c687ffb8fcf",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-07-24T01:29:01.327898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_autoencoder(autoencoder, device, epochs=50, batch_size=20, beta_1 = 0.8, beta_2 = 0.99):\n",
    "  \"\"\"\n",
    "  Function to train autoencoder\n",
    "\n",
    "  Args:\n",
    "    autoencoder: nn.module\n",
    "      Autoencoder instance\n",
    "    dataset: function\n",
    "      Dataset\n",
    "    device: string\n",
    "      GPU if available. CPU otherwise\n",
    "    epochs: int\n",
    "      Number of epochs [default: 20]\n",
    "    batch_size: int\n",
    "      Batch size\n",
    "    seed: int\n",
    "      Set seed for reproducibility; [default: 0]\n",
    "\n",
    "  Returns:\n",
    "    mse_loss: float\n",
    "      MSE Loss\n",
    "  \"\"\"\n",
    "  autoencoder.to(device)\n",
    "  optim = torch.optim.Adam(autoencoder.parameters(),\n",
    "                           lr=0.001,\n",
    "                           weight_decay=1e-5,\n",
    "                           betas = (beta_1, beta_2))\n",
    "  loss_fn = nn.MSELoss()\n",
    "  train_mse_loss = []\n",
    "  valid_mse_loss = []\n",
    "  \n",
    "  i = 0\n",
    "  for epoch in range(epochs):\n",
    "    for data, target in train_loader:\n",
    "      data = data.to(device).float()\n",
    "      optim.zero_grad()\n",
    "      # im_batch = torch.mean(im_batch, dim=0)\n",
    "      reconstruction = autoencoder.forward(data)\n",
    "      # Loss calculation\n",
    "      target = target.to(device).float()\n",
    "      loss = loss_fn(reconstruction.view(batch_size, -1),\n",
    "                     target.view(batch_size, -1))\n",
    "      loss.backward()\n",
    "      optim.step()\n",
    "\n",
    "      train_mse_loss.append(loss.detach())\n",
    "      \n",
    "        \n",
    "    for data, target in valid_loader:\n",
    "      data = data.to(device).float()\n",
    "      optim.zero_grad()\n",
    "      # im_batch = torch.mean(im_batch, dim=0)\n",
    "      reconstruction = autoencoder.forward(data)\n",
    "      # Loss calculation\n",
    "      target = target.to(device).float()\n",
    "      loss = loss_fn(reconstruction.view(batch_size, -1),\n",
    "                     target.view(batch_size, -1))\n",
    "\n",
    "      valid_mse_loss.append(loss.detach())\n",
    "    \n",
    "    i += 1\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Train MSE Loss: {train_mse_loss[i-1]}, Validation MSE loss: {valid_mse_loss[i-1]}')\n",
    "  # After training completes,\n",
    "  # make sure the model is on CPU so we can easily\n",
    "  # do more visualizations and demos.  \n",
    "  autoencoder.to('cpu')\n",
    "  \n",
    "  return train_mse_loss, valid_mse_loss\n",
    "\n",
    "train_mse_loss, valid_mse_loss = train_autoencoder(trained_conv_AE, device=DEVICE)   "
   ],
   "id": "47d6098b1fe2fb60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train MSE Loss: 0.026964936405420303, Validation MSE loss: 0.03845549002289772\n",
      "Epoch 2/50, Train MSE Loss: 8.086341857910156, Validation MSE loss: 0.03856110945343971\n",
      "Epoch 3/50, Train MSE Loss: 0.02374318055808544, Validation MSE loss: 0.03954027593135834\n",
      "Epoch 4/50, Train MSE Loss: 0.04414782673120499, Validation MSE loss: 0.03681551665067673\n",
      "Epoch 5/50, Train MSE Loss: 0.06358170509338379, Validation MSE loss: 0.03865194320678711\n",
      "Epoch 6/50, Train MSE Loss: 0.08054539561271667, Validation MSE loss: 0.03992020711302757\n",
      "Epoch 7/50, Train MSE Loss: 0.09744232147932053, Validation MSE loss: 0.03819240629673004\n",
      "Epoch 8/50, Train MSE Loss: 0.10861942172050476, Validation MSE loss: 0.03869609162211418\n",
      "Epoch 9/50, Train MSE Loss: 0.11330688744783401, Validation MSE loss: 0.020786253735423088\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure()\n",
    "plt.plot(train_mse_loss)\n",
    "plt.legend(['train_loss'])\n",
    "plt.xlabel('train batch')\n",
    "plt.ylabel('MSE Loss')\n",
    "# plt.ylim(0,1)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(valid_mse_loss)\n",
    "plt.legend(['validation loss'])\n",
    "plt.xlabel('validation batch')\n",
    "plt.ylabel('MSE Loss')\n",
    "#plt.ylim([0,2*torch.as_tensor(conv_losses).median()])\n",
    "plt.show()"
   ],
   "id": "aa2f436492637b84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def test_model(test_loader, model):\n",
    "    \n",
    "    test_mse_loss = []\n",
    "    \n",
    "    for data, target in test_loader: \n",
    "        data = data.float()\n",
    "        # im_batch = torch.mean(im_batch, dim=0)\n",
    "        loss_fn = nn.MSELoss()\n",
    "        batch_size = 20\n",
    "        reconstruction = model.forward(data)\n",
    "        # Loss calculation\n",
    "        loss = loss_fn(reconstruction.view(batch_size, -1),target.view(batch_size, -1))\n",
    "\n",
    "        test_mse_loss.append(loss.detach())\n",
    "    \n",
    "    return test_mse_loss\n",
    "\n",
    "test_mse_lost = test_model(test_loader, trained_conv_AE)"
   ],
   "id": "9dba3041585d3f68",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
