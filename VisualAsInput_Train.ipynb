{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# import torchvision.transforms as transforms\n",
    "# import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def set_device():\n",
    "    \"\"\"\n",
    "    Set the device. CUDA if available, otherwise CPU.\n",
    "    \n",
    "    Args: None\n",
    "    \n",
    "    Returns: Nothing   \n",
    "    \"\"\"\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device != \"cuda\":\n",
    "        print(\"This notebook will be running on CPU!\")\n",
    "    else: \n",
    "        print(\"You have CUDA set up! Woohoo!!!\")\n",
    "        \n",
    "    return device\n",
    "DEVICE = set_device()"
   ],
   "id": "1816971feffb433",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os, requests\n",
    "\n",
    "fname = []\n",
    "for j in range(3):\n",
    "  fname.append('steinmetz_part%d.npz'%j)\n",
    "url = [\"https://osf.io/agvxh/download\"]\n",
    "url.append(\"https://osf.io/uv3mw/download\")\n",
    "url.append(\"https://osf.io/ehmw2/download\")\n",
    "\n",
    "for j in range(len(url)):\n",
    "  if not os.path.isfile(fname[j]):\n",
    "    try:\n",
    "      r = requests.get(url[j])\n",
    "    except requests.ConnectionError:\n",
    "      print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "      if r.status_code != requests.codes.ok:\n",
    "        print(\"!!! Failed to download data !!!\")\n",
    "      else:\n",
    "        with open(fname[j], \"wb\") as fid:\n",
    "          fid.write(r.content)\n",
    "          \n",
    "alldat = np.array([])\n",
    "for j in range(len(fname)):\n",
    "  alldat = np.hstack((alldat,\n",
    "                      np.load('steinmetz_part%d.npz'%j,\n",
    "                              allow_pickle=True)['dat']))"
   ],
   "id": "fa6ff8b8b201cc95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "basal_ganglia_regions = [\"ACB\", \"CP\", \"GPe\", \"LS\", \"LSc\", \"LSr\", \"MS\", \"OT\", \"SNr\", \"SI\"]\n",
    "visual_cortex_regions = [\"VISa\", \"VISam\", \"VISl\", \"VISp\", \"VISpm\", \"VISrl\"]\n",
    "\n",
    "total_vc_neurons=0\n",
    "total_bg_neurons=0\n",
    "\n",
    "sessions_to_use = []  ##Session indices\n",
    "session_lengths = []  ##The number of trials in each session\n",
    "input_visual_tensor_list = []\n",
    "label_basal_tensor_list = []\n",
    "\n",
    "for session in range(len(alldat)):\n",
    "  num_vis_neurons = 0\n",
    "  num_basal_neurons = 0\n",
    "  vis_neuron_indices = []\n",
    "  basal_neuron_indices = []\n",
    "  for neuron_idx, area in enumerate(alldat[session]['brain_area']):\n",
    "    if area in visual_cortex_regions:  ## Viusal areas\n",
    "      num_vis_neurons += 1\n",
    "      vis_neuron_indices.append(neuron_idx)\n",
    "    if area in basal_ganglia_regions:  ## Basal Ganglia\n",
    "      num_basal_neurons += 1\n",
    "      basal_neuron_indices.append(neuron_idx)\n",
    "          \n",
    "  trial_num = alldat[session]['spks'].shape[1]   \n",
    "  input_visual_recordings = np.empty((num_vis_neurons, trial_num, 250))\n",
    "  label_basal_recordings = np.empty((num_basal_neurons, trial_num, 250))\n",
    "  \n",
    "  \n",
    "  if num_vis_neurons*num_basal_neurons != 0:\n",
    "    sessions_to_use.append(session)\n",
    "    print(\"session \" ,session, \"has both visual and basal!\")\n",
    "    print('The name of the mouse: ' + alldat[session]['mouse_name'])\n",
    "    print('The session time' + alldat[session]['date_exp'])\n",
    "    print(alldat[session]['spks'].shape)\n",
    "\n",
    "    print('The number of neurons being recorded:', len(alldat[session]['spks']))\n",
    "    print('Number of visual neurons: ', num_vis_neurons)\n",
    "    # print('Indices of visual neurons: ', vis_neuron_indices)\n",
    "    \n",
    "    ## To extract the data from alldat[current_session]\n",
    "    for i, v in enumerate(vis_neuron_indices): \n",
    "      input_visual_recordings[i, :, :] = alldat[session]['spks'][v, :, :]\n",
    "    print(\"reorganized visual activities for input: \", input_visual_recordings.shape)\n",
    "    print('Number of basal neurons: ', num_basal_neurons)\n",
    "    # print('Indices of basal neurons: ', basal_neuron_indices)\n",
    "    for i, g in enumerate(basal_neuron_indices):\n",
    "      label_basal_recordings[i, :, :] = alldat[session]['spks'][g, :, :]\n",
    "    print(\"reorganized basal activities for label\", label_basal_recordings.shape)\n",
    "    print('----------------------------------------------------')\n",
    "    \n",
    "    visual_input_tensor = torch.from_numpy(input_visual_recordings).reshape(-1, 250)\n",
    "    input_visual_tensor_list.append(visual_input_tensor)\n",
    "    basal_label_tensor = torch.from_numpy(label_basal_recordings).reshape(-1, 250)\n",
    "    label_basal_tensor_list.append(basal_label_tensor)\n",
    "\n",
    "# input = torch.cat(input_visual_tensor_list, axis = 0)\n",
    "# label = torch.cat(label_basal_tensor_list, axis = 0)\n",
    "# !!!!!! I realized that the firing rate across different sessions have huge variations, so it may not be a good idea to combine them together!!!!!\\\n",
    "\n",
    "print(\"input tensor list: We have\", len(input_visual_tensor_list), \"and their shapes are \", [input_visual_tensor_list[i].shape for i in range(len(input_visual_tensor_list))])\n",
    "print(\"label tensor list: We have\", len(label_basal_tensor_list), \"and their shapes are\", [label_basal_tensor_list[i].shape for i in range(len(label_basal_tensor_list))])\n"
   ],
   "id": "22bf32dfdeb269d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`alldat` contains 39 sessions from 10 mice, data from Steinmetz et al, 2019. Time bins for all measurements are 10ms, starting 500ms before stimulus onset. The mouse had to determine which side has the highest contrast. For each `dat = alldat[k]`, you have the fields below. For extra variables, check out the extra notebook and extra data files (lfp, waveforms and exact spike times, non-binned). \n",
    "\n",
    "* `dat['mouse_name']`: mouse name\n",
    "* `dat['date_exp']`: when a session was performed\n",
    "* `dat['spks']`: neurons by trials by time bins.    \n",
    "* `dat['brain_area']`: brain area for each neuron recorded. \n",
    "* `dat['ccf']`: Allen Institute brain atlas coordinates for each neuron. \n",
    "* `dat['ccf_axes']`: axes names for the Allen CCF. \n",
    "* `dat['contrast_right']`: contrast level for the right stimulus, which is always contralateral to the recorded brain areas.\n",
    "* `dat['contrast_left']`: contrast level for left stimulus. \n",
    "* `dat['gocue']`: when the go cue sound was played. \n",
    "* `dat['response_time']`: when the response was registered, which has to be after the go cue. The mouse can turn the wheel before the go cue (and nearly always does!), but the stimulus on the screen won't move before the go cue.  \n",
    "* `dat['response']`: which side the response was (`-1`, `0`, `1`). When the right-side stimulus had higher contrast, the correct choice was `-1`. `0` is a no go response. \n",
    "* `dat['feedback_time']`: when feedback was provided. \n",
    "* `dat['feedback_type']`: if the feedback was positive (`+1`, reward) or negative (`-1`, white noise burst).  \n",
    "* `dat['wheel']`: turning speed of the wheel that the mice uses to make a response, sampled at `10ms`. \n",
    "* `dat['pupil']`: pupil area  (noisy, because pupil is very small) + pupil horizontal and vertical position.\n",
    "* `dat['face']`: average face motion energy from a video camera. \n",
    "* `dat['licks']`: lick detections, 0 or 1.   \n",
    "* `dat['trough_to_peak']`: measures the width of the action potential waveform for each neuron. Widths `<=10` samples are \"putative fast spiking neurons\". \n",
    "* `dat['%X%_passive']`: same as above for `X` = {`spks`, `pupil`, `wheel`, `contrast_left`, `contrast_right`} but for  passive trials at the end of the recording when the mouse was no longer engaged and stopped making responses. \n",
    "* `dat['prev_reward']`: time of the feedback (reward/white noise) on the previous trial in relation to the current stimulus time. \n",
    "* `dat['reaction_time']`: ntrials by 2. First column: reaction time computed from the wheel movement as the first sample above `5` ticks/10ms bin. Second column: direction of the wheel movement (`0` = no move detected).  \n",
    "\n",
    "\n",
    "The original dataset is here: https://figshare.com/articles/dataset/Dataset_from_Steinmetz_et_al_2019/9598406"
   ],
   "id": "69afd360d00ec9e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### 2024/07/15 We set the turncation threshold to be 50 neurons\n",
    "## adding reshape methods -- please go over the code to see if modified/added \"aspects\" are consistent \n",
    "## and do what they are intended to do, the number of neurons went down (perhaps a little bit more than intended)\n",
    "## and the same for the trial number, but now they match in dimensions\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    I tried to make it tailored to our needs.\n",
    "    It only considers sessions that have both visual and basal ganglia recordings.\n",
    "    It only considers go trials.\n",
    "    When indexed, it returns basal ganglia spikes and action).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        visual_cortex_regions: list = [\"VISa\", \"VISam\", \"VISl\", \"VISp\", \"VISpm\", \"VISrl\"], # Put them as default argument so they can be easily changed if we decide to change areas\n",
    "        basal_ganglia_regions: list = [\"ACB\", \"CP\", \"GPe\", \"LS\", \"LSc\", \"LSr\", \"MS\", \"OT\", \"SNr\", \"SI\"],\n",
    "        thresh=50\n",
    "    ) -> None:\n",
    "        \n",
    "        self.visual_cortex_regions = visual_cortex_regions\n",
    "        self.basal_ganglia_regions = basal_ganglia_regions\n",
    "        self.thresh=thresh\n",
    "        self.data = self.visual_and_basal(data)\n",
    "        self.data = self.get_go_trials(self.data)\n",
    "        self.min_trials, self.min_neurons = self.find_min_dimensions(self.data) # new \n",
    "        print(f\"Minimum number of trials: {self.min_trials}\")\n",
    "        print(f\"Minimum number of neurons: {self.thresh}\")\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        session_index: int\n",
    "    ) -> torch.tensor:\n",
    "        \n",
    "        session = self.data[session_index]\n",
    "        \n",
    "        # We need visual spikes now!!!\n",
    "        input_visual_spks = self.get_truncated_spikes(session, self.visual_cortex_regions)\n",
    "        \n",
    "        ## Some number checks by Yangdong 24/7/15\n",
    "        ## print(\"the shape of spikes_visual after truncating:\", spikes_visual.shape)\n",
    "\n",
    "        # input_basal_spks = self.get_truncated_spikes(session, self.basal_ganglia_regions) # This will be the input data (time, trials, eurons_indicies)          \n",
    "        label_action = torch.tensor(session['response'][0:self.min_trials]) #(-1: the mouse is turning the wheel to the left, which indicates that the mouse thinks the right side has higher contrast level. )\n",
    "        \n",
    "        return input_visual_spks, label_action\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.data)\n",
    "\n",
    "    def visual_and_basal(\n",
    "        self,\n",
    "        data: np.array\n",
    "    ) -> list:\n",
    "        \"\"\"\n",
    "        It checks whether any of the considered brain areas recorded in the session\n",
    "        is present in both visual or basal regions.\n",
    "        Also excludes sessions with a number of neurons < self.threshold\n",
    "        \"\"\"\n",
    "      \n",
    "        visual_and_basal_sessions = []\n",
    "        for session in data:\n",
    "        \n",
    "            visual_ok = any(area in self.visual_cortex_regions for area in session[\"brain_area\"]) \n",
    "            basal_ok = any(area in self.basal_ganglia_regions for area in session[\"brain_area\"]) \n",
    "            vis_neuron_count = len([i for i, area in enumerate(session[\"brain_area\"]) if area in self.visual_cortex_regions])\n",
    "            bg_neuron_count = len([i for i, area in enumerate(session[\"brain_area\"]) if area in self.basal_ganglia_regions])\n",
    "            min_count = np.min([vis_neuron_count, bg_neuron_count])\n",
    "\n",
    "            if visual_ok and basal_ok and min_count>self.thresh:\n",
    "                visual_and_basal_sessions.append(session)\n",
    "\n",
    "        return visual_and_basal_sessions  \n",
    "        ## a list of actual sessions (with all datas) which contains both, and the number of neurons is larger than the threshold...\n",
    "\n",
    "    def get_go_trials(\n",
    "        self,\n",
    "        data: list\n",
    "    ) -> list:\n",
    "        \"\"\"\n",
    "        Within sessions, excludes no-go trials.\n",
    "        \"\"\"\n",
    "        sessions_with_go_trials = []\n",
    "        for session in data:\n",
    "            go_trial_indices = [i for i, response in enumerate(session[\"response\"]) if response != 0]\n",
    "            if go_trial_indices:\n",
    "                filtered_session = {} # sessions are dicts\n",
    "                for key, value in session.items():\n",
    "                    # Include all the keys of the session dictionary\n",
    "                    if isinstance(value, np.ndarray) and value.ndim > 1 and value.shape[1] == len(session[\"response\"]): # like \"spks\" is multidimensional\n",
    "                        filtered_session[key] = value[:, go_trial_indices]\n",
    "                    elif isinstance(value, np.ndarray) and len(value) == len(session[\"response\"]): # like \"response is 1 dimensional\"\n",
    "                        filtered_session[key] = value[go_trial_indices]\n",
    "                    else: # like \"mouse_name\" is a single value\n",
    "                        filtered_session[key] = value\n",
    "                sessions_with_go_trials.append(filtered_session)\n",
    "        \n",
    "        return sessions_with_go_trials  \n",
    "\n",
    "    def find_min_dimensions(\n",
    "        self,\n",
    "        data: list\n",
    "    ) -> tuple:\n",
    "        min_trials = float('inf')\n",
    "        min_neurons = float('inf')\n",
    "        \"\"\"\n",
    "        Helper function to establish \"\"\"        \n",
    "        \n",
    "        for session in data:\n",
    "            spikes = session[\"spks\"]  ## [neurons_number, trial_Indexes, time]\n",
    "            min_trials = min(min_trials, spikes.shape[1])\n",
    "            # min_neurons = min(min_neurons, spikes.shape[0])\n",
    "        \n",
    "        return min_trials, min_neurons\n",
    "\n",
    "    def get_truncated_spikes(\n",
    "        self,\n",
    "        session: dict,\n",
    "        regions: list\n",
    "    ) -> torch.tensor:\n",
    "\n",
    "        neuron_indices = [i for i, area in enumerate(session[\"brain_area\"]) if area in regions]\n",
    "        spikes = session[\"spks\"][neuron_indices, :, :]  ## containing spks from visual/basal areas, e.g 17\n",
    "        spikes = spikes[:self.thresh, :, :] \n",
    "        spikes = spikes[:, :self.min_trials, :]\n",
    "\n",
    "        spikes = torch.tensor(spikes)\n",
    "        spikes = torch.permute(spikes, (2,0,1))  ## time, neuron_index, trial_index\n",
    "\n",
    "        # 24/07/15 -> Dario: should we leave the conversion to tensor in the __getitem__ method?\n",
    "\n",
    "        return spikes\n",
    "    \n",
    "    def sanity_check(self):\n",
    "        for idx, session in enumerate(self.data):\n",
    "            spikes_visual = self.get_truncated_spikes(session, self.visual_cortex_regions)\n",
    "            spikes_basal = self.get_truncated_spikes(session, self.basal_ganglia_regions)\n",
    "\n",
    "            assert spikes_visual.shape[1] == self.min_trials, f\"Session idx visual cortex trials mismatch.\"\n",
    "            assert spikes_visual.shape[2] == self.min_neurons, f\"Session idx visual cortex neurons mismatch.\"\n",
    "            assert spikes_basal.shape[1] == self.min_trials, f\"Session idx basal ganglia trials mismatch.\"\n",
    "            assert spikes_basal.shape[2] == self.min_neurons, f\"Session idx basal ganglia neurons mismatch.\"\n",
    "    print(\"Sanity check clear\")\n",
    "\n",
    "thresh=50\n",
    "all_data = CustomDataset(alldat)\n",
    "x = all_data[0]\n",
    "\n",
    "print(\"We have\", len(all_data), \"number of sessions.\")\n",
    "print(\"Each of the session has\", all_data[0][0].shape[2], \"trials, and\", all_data[0][0].shape[1], \"neurons.\")\n",
    "print(\"We have\", 123*6, \"input basal spike matrx.\")"
   ],
   "id": "62accc51c5d0d330",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# a = (all_data[0][0], all_data[0][1])  # Initialize 'a' with the first tuple's tensors.\n",
    "\n",
    "merged = all_data[0]\n",
    "\n",
    "for i, session in enumerate(all_data):  # Start from the second element.\n",
    "    # print(i)\n",
    "    if i == 0: continue\n",
    "    merged = (torch.cat((merged[0], session[0]), dim=2),  # Concatenate along dim=2 for the first tensor.\n",
    "         torch.cat((merged[1], session[1])))         # Simply concatenate along the last dimension for the second tensor.\n",
    "\n",
    "print(merged[0].shape, merged[1].shape)"
   ],
   "id": "60b409c9c1e3ef01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import SubsetRandomSampler, DataLoader, Dataset\n",
    "\n",
    "class Session(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_spikes = self.data[0][:, :, idx] # input_spikes has the shape (time, neuron_index)\n",
    "        label_action = self.data[1][idx] # has the action tensor (1) or (-1).\n",
    "        return input_spikes, label_action\n",
    "\n",
    "def split(session, validation_split=0.2, shuffle_trials=True, batch_size=1):\n",
    "    num_trials = session[0].shape[2]\n",
    "    indices = list(range(num_trials))\n",
    "    split_index = int(np.floor(validation_split * num_trials))\n",
    "    if shuffle_trials:\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split_index:], indices[:split_index]\n",
    "    \n",
    "    # Creating PT data samplers and loaders\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "    \n",
    "    session_dataset = Session(session)\n",
    "    \n",
    "    train_loader = DataLoader(session_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    valid_loader = DataLoader(session_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "    \n",
    "    return train_loader, valid_loader\n",
    "\n",
    "train_loader, valid_loader = split(merged)\n",
    "\n",
    "# for batch_index, (batch_neural_recordings, batch_actions) in enumerate(train_loader):\n",
    "#     print(f\"Batch index: {batch_index}:\")\n",
    "#     print(f\"batch_neural_recordings shape: {batch_neural_recordings.shape}\")\n",
    "#     print(f\"Batch_actions: {batch_actions}\")\n",
    "#     \n",
    "print(\"-------------------------------------------------------------------\")\n",
    "# \n",
    "# for batch_index, (batch_neural_recordings, batch_actions) in enumerate(valid_loader):\n",
    "#     print(f\"Batch index: {batch_index}:\")\n",
    "#     print(f\"batch_neural_recordings shape: {batch_neural_recordings.shape}\")\n",
    "#     print(f\"Batch_actions: {batch_actions}\")"
   ],
   "id": "e90f98c5ca13c13a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        # self.criterion = nn.CrossEntropyLoss()\n",
    "        # self.learning_rate = 0.001\n",
    "        # self.optimizer = torch.optim.Adam(self.model.parameters(), lr = self.learning_rate)\n",
    "        # First define the layers.\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=2) \n",
    "        # Input 1 channel, output 6 channels, referring to the high level projection to the cortex.\n",
    "        # self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
    "        self.fc1 = nn.Linear(50 * 250 * 6, 2) # projecting onto the action space, which only contains left (-1) or right (1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input basal activity (250, 50) as (batch_size, time, neurons)\n",
    "        \"\"\"\n",
    "        # Conv layer 1.\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Fully connected layer 1.\n",
    "        x = x.view(-1, 250 * 50 * 6)  # You have to first flatten the output from the previous convolution layer.\n",
    "        x = self.fc1(x)\n",
    "        # x = torch.nn.functional.softmax(x, dim=1)\n",
    "        # Cross Entropy already contains softmax computation so we dont need that here anymore! \n",
    "        return x  ## Returning the logits\n",
    "    "
   ],
   "id": "bd4a93f27479400e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "motion_decoder = decoder()\n",
    "motion_decoder.load_state_dict(torch.load('/home/wyd716/PycharmProjects/NMA_DL_NeuronPrediction/Motion_decoder_AllSession_Merged.pt'))\n",
    "motion_decoder.requires_grad_ = False\n",
    "motion_decoder.eval()"
   ],
   "id": "5bd4d64e17d8c987",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Evo_encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, motion_decoder):\n",
    "        \n",
    "        super().__init__()\n",
    "        # self.criterion = nn.CrossEntropyLoss()\n",
    "        # self.learning_rate = 0.001\n",
    "        # self.optimizer = torch.optim.Adam(self.model.parameters(), lr = self.learning_rate)\n",
    "        # First define the layers.\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=2) \n",
    "        self.vtom = nn.Conv2d(6, 6, kernel_size=5, padding=2) # The projection from Visual Cortex to Motor cortex\n",
    "        self.vtohtm = nn.Conv2d(6, 2, kernel_size=5, padding=2) # The projection to the hypothalamus \n",
    "        self.conv2 = nn.Conv2d(8, 1, kernel_size=5, padding=2) # Combining the mortor cortex and hypothalamus\n",
    "        self.MotionDecoder = motion_decoder # projecting onto the action space, which only contains left (-1) or right (1)\n",
    "        motion_decoder.training = False\n",
    "        motion_decoder.conv1.bias.requires_grad = False\n",
    "        motion_decoder.conv1.weight.requires_grad = False\n",
    "        motion_decoder.fc1.bias.requires_grad = False\n",
    "        motion_decoder.fc1.weight.requires_grad = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input basal activity (250, 50) as (batch_size, time, neurons)\n",
    "        \"\"\"\n",
    "        # Conv layer 1.\n",
    "        x = F.relu(self.conv1(x)) # The shape of this output is (1, 6, 250, 50)\n",
    "        m = F.relu(self.vtom(x))\n",
    "        htm = F.relu(self.vtohtm(x))\n",
    "        # Fully connected layer 1.\n",
    "        x = torch.cat((m, htm), dim=1)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.MotionDecoder(x) # The input here need to be (250,50)\n",
    "        # x = torch.nn.functional.softmax(x, dim=1)\n",
    "        # Cross Entropy already contains softmax computation so we dont need that here anymore! \n",
    "        return x\n",
    "    \n",
    "Evo_encoder = Evo_encoder(motion_decoder)"
   ],
   "id": "d2f7d6f9a90855bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "Evo_encoder.eval()",
   "id": "b2393e8f7bdc5e50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train(model, device, epochs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Training loop\n",
    "    \n",
    "    Args:\n",
    "    model: nn.module\n",
    "      Neural network instance\n",
    "    device: string\n",
    "      GPU/CUDA if available, CPU otherwise\n",
    "    epochs: int\n",
    "      Number of epochs\n",
    "    train_loader: torch.loader\n",
    "      Training Set\n",
    "    validation_loader: torch.loader\n",
    "      Validation set\n",
    "    \n",
    "    Returns:\n",
    "    Nothing\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        device = torch.device('cuda')\n",
    "        model.to(device)\n",
    "    \n",
    "    criterion =  nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "    train_loss, validation_loss = [], []\n",
    "    train_acc, validation_acc = [], []\n",
    "\n",
    "    \n",
    "    # running_loss_train = 0.0\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "            \n",
    "       train_loader, valid_loader = split(merged)\n",
    "       correct, total = 0, 0\n",
    "       running_loss = 0.0\n",
    "       #### Looping through all the trials\n",
    "       for data, target in train_loader:     \n",
    "           \n",
    "           data = data.float()\n",
    "           # print(data.shape)\n",
    "           data = data.unsqueeze(1)\n",
    "           data, target = data.to(device), target.to(device)\n",
    "           output = model.forward(data) # Here the output is the normalized probability\n",
    "           # print(output.shape)\n",
    "           optimizer.zero_grad()\n",
    "           target = (target + 1) // 2 # Here we are transferring the -1 target to 0\n",
    "           target = target.long()  # Not sure what this does but this is what traceback wanted\n",
    "           loss = criterion(output, target)\n",
    "           running_loss += loss\n",
    "           loss.backward()\n",
    "           optimizer.step()\n",
    "           total += 1\n",
    "           # Get accuracy\n",
    "           _, predicted = torch.max(output, dim = 1) #Get the index of output, which is exactly what the model predicted. \n",
    "           # total += target.size(0) # Commenting this out because we are not running butches now\n",
    "           if predicted == target: \n",
    "               correct += 1 # revised for no-batches running\n",
    "    \n",
    "       train_loss.append( running_loss.cpu().detach().float()) # For every trial we are appending the accumulated loss\n",
    "       train_acc.append( (correct/total)*100 ) # accumulating the percentage of correct prediction after each trial being trained.\n",
    "           \n",
    "           # model.eval()\n",
    "       \n",
    "       # print(\"---------------\")\n",
    "       # print(\"Finished training\", len(train_loader), \"training trials in the session\", s+1)\n",
    "       # print(\"Accuracy on training set:\", train_acc[-1])\n",
    "       \n",
    "       correct, total = 0, 0\n",
    "       running_loss = 0.0\n",
    "       for data, target in valid_loader:\n",
    "           \n",
    "           optimizer.zero_grad()\n",
    "           data = data.float()\n",
    "           data = data.unsqueeze(1)\n",
    "           data, target = data.to(device), target.to(device)\n",
    "           output = model.forward(data)\n",
    "           target = (target + 1) // 2\n",
    "           target = target.long()\n",
    "           loss = criterion(output, target)\n",
    "           running_loss += loss\n",
    "           # We will not be calculating gradient here!!!\n",
    "           _, predicted = torch.max(output, dim = 1) # Getting the predicted outcome\n",
    "           # total += target.size(0)\n",
    "           if predicted == target: correct += 1\n",
    "           total += 1\n",
    "        \n",
    "       validation_loss.append( running_loss.cpu().detach().float() ) # For every trial we are appending the accumulated loss\n",
    "       validation_acc.append( correct/total*100 ) # accumulating the percentage of correct prediction after each trial being trained.\n",
    "       \n",
    "       if epoch % 5 == 0: \n",
    "           print(\"Epoch:\", epoch, \", the train loss:\", train_loss[-1], \", the validation loss:\", validation_loss[-1])\n",
    "    \n",
    "    return train_loss, validation_loss, train_acc, validation_acc\n",
    "\n",
    "# set_seed(2)\n",
    "## Uncomment to test your training loop\n",
    "\n",
    "Evo_train_loss, Evo_validation_loss, Evo_train_acc, Evo_validation_acc = train(Evo_encoder, \"cuda\", 250)\n"
   ],
   "id": "db4add9a99b73f3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_loss_accuracy(train_loss, train_acc,\n",
    "                       validation_loss, validation_acc):\n",
    "  \"\"\"\n",
    "  Code to plot loss and accuracy\n",
    "\n",
    "  Args:\n",
    "    train_loss: list\n",
    "      Log of training loss\n",
    "    validation_loss: list\n",
    "      Log of validation loss\n",
    "    train_acc: list\n",
    "      Log of training accuracy\n",
    "    validation_acc: list\n",
    "      Log of validation accuracy\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  epochs = len(train_loss)\n",
    "  fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "  ax1.plot(list(range(epochs)), train_loss, label='Training Loss')\n",
    "  ax1.plot(list(range(epochs)), validation_loss, label='Validation Loss')\n",
    "  ax1.set_xlabel('sessions')\n",
    "  ax1.set_ylabel('Loss')\n",
    "  ax1.set_title('Epoch vs Loss')\n",
    "  ax1.legend()\n",
    "\n",
    "  ax2.plot(list(range(epochs)), train_acc, label='Training Accuracy')\n",
    "  ax2.plot(list(range(epochs)), validation_acc, label='Validation Accuracy')\n",
    "  ax2.set_xlabel('sessions')\n",
    "  ax2.set_ylabel('Accuracy')\n",
    "  ax2.set_title('Epoch vs Accuracy')\n",
    "  ax2.legend()\n",
    "  fig.set_size_inches(15.5, 5.5)\n",
    "plot_loss_accuracy(Evo_train_loss, Evo_train_acc, Evo_validation_loss , Evo_validation_acc)"
   ],
   "id": "773212db7cb9b6ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Below is the encoder without any motion decoder evolution",
   "id": "a07fb18b840b075e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        # self.criterion = nn.CrossEntropyLoss()\n",
    "        # self.learning_rate = 0.001\n",
    "        # self.optimizer = torch.optim.Adam(self.model.parameters(), lr = self.learning_rate)\n",
    "        # First define the layers.\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=2) \n",
    "        self.vtom = nn.Conv2d(6, 6, kernel_size=5, padding=2) # The projection from Visual Cortex to Motor cortex\n",
    "        self.vtohtm = nn.Conv2d(6, 2, kernel_size=5, padding=2) # The projection to the hypothalamus \n",
    "        self.conv2 = nn.Conv2d(8, 1, kernel_size=5, padding=2) # Combining the mortor cortex and hypothalamus\n",
    "        self.fc1 = nn.Linear(250*50, 2)\n",
    "        self.fc1.training = False\n",
    "        self.fc1.bias.requires_grad_ = False\n",
    "        self.fc1.weight.requires_grad = False\n",
    "        # self.MotionDecoder = motion_decoder # projecting onto the action space, which only contains left (-1) or right (1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input basal activity (250, 50) as (batch_size, time, neurons)\n",
    "        \"\"\"\n",
    "        # Conv layer 1.\n",
    "        x = F.relu(self.conv1(x)) # The shape of this output is (1, 6, 250, 50)\n",
    "        m = F.relu(self.vtom(x))\n",
    "        htm = F.relu(self.vtohtm(x))\n",
    "        # Fully connected layer 1.\n",
    "        x = torch.cat((m, htm), dim=1)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1, 250 * 50)\n",
    "        x = self.fc1(x) # The input here need to be (250,50)\n",
    "        # x = torch.nn.functional.softmax(x, dim=1)\n",
    "        # Cross Entropy already contains softmax computation so we dont need that here anymore! \n",
    "        return x\n",
    "    \n",
    "encoder = encoder()"
   ],
   "id": "dc4867480872f6da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_loss, validation_loss, train_acc, validation_acc = train(encoder, \"cuda\", 250)",
   "id": "bd5a37d34037b394",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_loss_accuracy(train_loss, train_acc, validation_loss , validation_acc)",
   "id": "cbb23099e9ac9756",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Evo_encoder_total_params_withgrad = sum(a.numel() for a in Evo_encoder.parameters() if a.requires_grad)\n",
    "Evo_encoder_total_params = sum(b.numel() for b in Evo_encoder.parameters())\n",
    "print(\"The number of parameter being optimized by the Evolution-pressured encoding model is \", Evo_encoder_total_params_withgrad)\n",
    "print(\"The total number of parameter in the Evolution-pressured encoding model is \", Evo_encoder_total_params)\n",
    "encoder_total_params_withgrad = sum(c.numel() for c in encoder.parameters() if c.requires_grad)\n",
    "encoder_total_params = sum(d.numel() for d in encoder.parameters())\n",
    "print(\"The number of parameter being optimized by vanilla encoding model is \", encoder_total_params_withgrad)\n",
    "print(\"The total number of parameter in the vanilla encoding model is \", encoder_total_params)\n"
   ],
   "id": "2f094abbf9c17f43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.save(Evo_encoder.state_dict(), \"Evo_encoder.pt\")\n",
    "torch.save(encoder.state_dict(), \"encoder.pt\")"
   ],
   "id": "b7e2b030abc596d2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
